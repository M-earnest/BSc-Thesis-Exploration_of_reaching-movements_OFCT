import pandas as pd
import os
import errno  # handy system and path functions
import sys  # to get file system encoding
import glob
import locale
import numpy as np
import matplotlib.pyplot as plt
import scipy.linalg
import seaborn as sns

from mpl_toolkits.mplot3d import Axes3D
from matplotlib.lines import Line2D

from scipy import signal
from scipy import stats


def get_descriptive_stats_2H(subject_parameters, event_file):
    '''
    Extract numbers of failed and successful trials for each hand and append
    to subject-parameter DataFrame.

    input:
    subject_parameters = Dataframe of event_file generated by [Exp.py]
    Output:
    Dataframe
    '''
    df_descriptives = subject_parameters
    sucesses_r = []
    failures_r = []
    sucesses_l = []
    failures_l = []

    for idx, i in enumerate(event_file['result_r']):
        if event_file['result_r'][idx] == 'success':
            sucesses_r.append(event_file['result_r'][idx])
        elif event_file['result_r'][idx] == 'failure':
            failures_r.append(event_file['result_r'][idx])

    num_failures_r = len(failures_r)
    num_successes_r = len(sucesses_r)
    percentage_correct_r = num_successes_r/(num_successes_r+num_failures_r)

    df_descriptives['failures_r'] = len(failures_r)
    df_descriptives['sucesses_r'] = len(sucesses_r)
    df_descriptives['percentage_corr_r'] = percentage_correct_r

    for idx, i in enumerate(event_file['result_l']):
        if event_file['result_l'][idx] == 'success':
            sucesses_l.append(event_file['result_l'][idx])
        elif event_file['result_l'][idx] == 'failure':
            failures_l.append(event_file['result_l'][idx])
    num_failures_l = len(failures_l)
    num_successes_l = len(sucesses_l)
    percentage_correct_l = num_successes_l/(num_successes_l+num_failures_l)

    df_descriptives['failures_l'] = len(failures_l)
    df_descriptives['sucesses_l'] = len(sucesses_l)

    df_descriptives['percentage_corr_l'] = percentage_correct_l

    return df_descriptives

def group_by_result_2H(event_file,hand):
    '''
    Divide Trials by Success and return a Dataframe for faile and successfull trials each
    input:
    event_file = Dataframe of event_file generated by [Exp.py]
                (contains reaction and presentation times etc.)
    hand = hand of interest either "r" or "l" as string
    output:
    successful_trials = DataFrame containing info on successful trials
    failed_trials = DataFrame containing info on failed trials
    '''
    successful_trials = pd.DataFrame()
    failed_trials = pd.DataFrame()
    for result, data in event_file.groupby('result_'+hand):
#         print(result)
    #     if result == 'sucess':
        if result == 'sucess':
            successful_trials = data
        elif result == 'failure':
            failed_trials = data

    successful_trials = successful_trials.reset_index()
    failed_trials = failed_trials.reset_index()

    return successful_trials, failed_trials


def group_by_size(event_file):
    '''
    Divide Trials by Size of target stimuli and return a Dataframe for each possible size
    input:
    event_file = Dataframe of event_file generated by [Exp.py]
                (contains reaction and presentation times etc.)
    hand = hand of interest either "r" or "l" as string
    output:
    s1 = DataFrame containing info on smallest Target Cube (15cm)
    s2 = DataFrame containing info on middle sized Target Cube (20cm)
    s3 = DataFrame containing info on largest Target Cube (25cm)
    '''
    # grouping one_hand_task size
    s1 = pd.DataFrame()
    s2 = pd.DataFrame()
    s3 = pd.DataFrame()

    data = None

    for size_id, data in event_file.groupby('box_size_id'):
        if size_id == 's1':
            if s1.empty == False:
                s1 = s1.append(data)
            elif s1.empty:
                s1 = data
        if size_id == 's2':
            if s2.empty == False:
                s2 = s2.append(data)
            elif s2.empty:
                s2 = data
        if size_id == 's3':
            if s3.empty == False:
                s3 = s3.append(data)
            elif s3.empty:
                s3 = data
    return s1,s2,s3


#### Individual Positions

def group_by_position_2H(event_file, hand):
    '''
    Divide Trials by position of target stimuli and return a Dataframe for each possible position
    input:
    event_file = Dataframe of event_file generated by [Exp.py]
                (contains reaction and presentation times etc.)
    hand = hand of interest either "r" or "l" as string
    output:
    r1 = DataFrame containing info on right hand Target position at approx. head height
    m1 = DataFrame containing info on middle Target position at approx. head height
    l1 = DataFrame containing info on left hand Target position at approx. head height

    r2 = DataFrame containing info on right hand Target position at approx. shoulder height
    m2 = DataFrame containing info on middle Target position at approx. shoulder height
    l2 = DataFrame containing info on left hand Target position at approx. shoulder height

    r3 = DataFrame containing info on right hand Target position at approx. hip height
    m3 = DataFrame containing info on middle Target position at approx. hip height
    l3 = DataFrame containing info on left hand Target position at approx. hip height
    '''
    r1 = pd.DataFrame()
    m1 = pd.DataFrame()
    l1 = pd.DataFrame()

    r2 = pd.DataFrame()
    m2 = pd.DataFrame()
    l2 = pd.DataFrame()

    r3 = pd.DataFrame()
    m3 = pd.DataFrame()
    l3 = pd.DataFrame()

    data = None

    for box_pos_id, data in event_file.groupby('box_pos_id'+'_'+hand):
#         print(box_pos_id)
        if box_pos_id ==  'r1':
            if r1.empty == False:
                r1 = r1.append(data)
            elif r1.empty:
                r1 = data
        elif box_pos_id == 'm1':
            if m1.empty == False:
                m1 = m1.append(data)
            elif m1.empty:
                m1 = data
        elif box_pos_id == 'l1':
            if l1.empty == False:
                l1 = l1.append(data)
            elif l1.empty:
                l1 = data
        elif box_pos_id == 'r2':
            if r2.empty == False:
                r2 = r2.append(data)
            elif r2.empty:
                r2 = data
        elif box_pos_id == 'm2':
            if m2.empty == False:
                m2 = m2.append(data)
            elif m2.empty:
                m2 = data
        elif box_pos_id == 'l2':
            if l2.empty == False:
                l2 = l2.append(data)
            elif l2.empty:
                l2 = data
        elif box_pos_id == 'r3':
            if r3.empty == False:
                r3 = r3.append(data)
            elif r3.empty:
                r3 = data
        elif box_pos_id == 'm3':
            if m3.empty == False:
                m3 = m3.append(data)
            elif m3.empty:
                m3 = data
        elif box_pos_id == 'l3':
            if l3.empty == False:
                l3 = l3.append(data)
            elif l3.empty:
                l3 = data
    r1 = r1.reset_index()
    m1 = m1.reset_index()
    l1 = l1.reset_index()

    r2 = r2.reset_index()
    m2 = m2.reset_index()
    l2 = l2.reset_index()

    r3 = r3.reset_index()
    m3 = m3.reset_index()
    l3 = l3.reset_index()

    return r1,r2,r3,m1,m2,m3,l1,l2,l3


def group_by_pos_both_hands(event_file):
    '''
    Divide Trials by combination of target stimuli position  for both hands
    and return a Dataframe for each possible combination
    input:
    event_file = Dataframe of event_file generated by [Exp.py]
                (contains reaction and presentation times etc.)
    output:
    r1_m1 = DataFrame containing info on right hand Target position (r)
            and middle target position (l) at approx. head height
    r1_l1 = DataFrame containing info on right hand Target position (r)
            and left target position (l) at approx. head height
    m1_l1 = DataFrame containing info on middle Target position (r)
            and left target position (l) at approx. head height

    r2_m2 = DataFrame containing info on right hand Target position (r)
            and middle target position (l) at approx. shoulder height
    r2_l2 = DataFrame containing info on right hand Target position (r)
            and left target position (l) at approx. shoulder height
    m2_l2 = DataFrame containing info on middle Target position (r)
            and left target position (l) at approx. shoulder height

    r3_m3 = DataFrame containing info on right hand Target position (r)
            and middle target position (l) at approx. hip height
    r3_l3 = DataFrame containing info on right hand Target position (r)
            and left target position (l) at approx. hip height
    m3_l3 = DataFrame containing info on middle Target position (r)
            and left target position (l) at approx. hip height

    r1_l3 = DataFrame containing info on right hand Target position (r) at approx. head height
            and left target position (l) at approx. hip height
    r3_l1 = DataFrame containing info on right hand Target position (r) at approx. hip height
            and left target position (l) at approx. head height
    m1_m3 = DataFrame containing info on middle Target position (r) at approx. head height
            and middle target position (l) at approx. hip height
    m3_m1 = DataFrame containing info on middle Target position (r) at approx. hip height
            and middle target position (l) at approx. head height
    '''
    # boxes at approx. head height
    r1_m1 = pd.DataFrame()
    r1_l1 = pd.DataFrame()
    m1_l1 = pd.DataFrame()

    # boxes at approx. chest height
    r2_l2 = pd.DataFrame()
    m2_l2 = pd.DataFrame()
    r2_m2 = pd.DataFrame()

    # boxes at approx. hip height
    r3_l3 = pd.DataFrame()
    m3_l3 = pd.DataFrame()
    r3_m3 = pd.DataFrame()

    # boxes at mixed heights
    r1_l3 = pd.DataFrame()
    r3_l1 = pd.DataFrame()
    m1_m3 = pd.DataFrame()
    m3_m1 = pd.DataFrame()

    data = None

    for box_pos_id, data in event_file.groupby('box_pos_id'):
        # boxes at approx. head height
        print(box_pos_id)
        if box_pos_id ==  'r1-m1':
            if r1_m1.empty == False:
                r1_m1 = r1_m1.append(data)
            elif r1_m1.empty:
                r1_m1 = data
        elif box_pos_id == 'r1-l1':
            if r1_l1.empty == False:
                r1_l1 = r1_l1.append(data)
            elif r1_l1.empty:
                r1_l1 = data
        elif box_pos_id == 'm1-l1':
            if m1_l1.empty == False:
                m1_l1 = m1_l1.append(data)
            elif m1_l1.empty:
                m1_l1 = data
        # boxes at approx. chest height
        if box_pos_id ==  'r2-m2':
            if r2_m2.empty == False:
                r2_m2 = r2_m2.append(data)
            elif r2_m2.empty:
                r2_m2 = data
        elif box_pos_id == 'r2-l2':
            if r2_l2.empty == False:
                r2_l2 = r2_l2.append(data)
            elif r2_l2.empty:
                r2_l2 = data
        elif box_pos_id == 'm2-l2':
            if m2_l2.empty == False:
                m2_l2 = m2_l2.append(data)
            elif m2_l2.empty:
                m2_l2 = data
        # boxes at approx. hip height
        if box_pos_id ==  'r3-m3':
            if r3_m3.empty == False:
                r3_m3 = r3_m3.append(data)
            elif r3_m3.empty:
                r3_m3 = data
        elif box_pos_id == 'r3-l3':
            if r3_l3.empty == False:
                r3_l3 = r3_l3.append(data)
            elif r3_l3.empty:
                r3_l3 = data
        elif box_pos_id == 'm3-l3':
            if m3_l3.empty == False:
                m3_l3 = m3_l3.append(data)
            elif m3_l3.empty:
                m3_l3 = data
        # boxes at mixed heights
        if box_pos_id ==  'r3-l1':
            if r3_l1.empty == False:
                r3_l1 = r3_l1.append(data)
            elif r3_l1.empty:
                r3_l1 = data
        elif box_pos_id == 'r1-l3':
            if r1_l3.empty == False:
                r1_l3 = r1_l3.append(data)
            elif r1_l3.empty:
                r1_l3 = data
        elif box_pos_id == 'm1-m3':
            if m1_m3.empty == False:
                m1_m3 = m1_m3.append(data)
            elif m1_m3.empty:
                m1_m3 = data
        elif box_pos_id == 'm3-m1':
            print('here')
            if m3_m1.empty == False:
                m3_m1 = m3_m1.append(data)
            elif m3_m1.empty:
                m3_m1 = data

    # boxes at approx. head height
    r1_m1 = r1_m1.reset_index()
    r1_l1 = r1_l1.reset_index()
    m1_l1 = m1_l1.reset_index()

    # boxes at approx. chest height
    r2_l2 = r2_l2.reset_index()
    m2_l2 = m2_l2.reset_index()
    r2_m2 = r2_m2.reset_index()

    # boxes at approx. hip height
    r3_l3 = r3_l3.reset_index()
    m3_l3 = m3_l3.reset_index()
    r3_m3 = r3_m3.reset_index()

    # boxes at mixed heights
    r1_l3 = r1_l3.reset_index()
    r3_l1 = r3_l1.reset_index()
    m1_m3 = m1_m3.reset_index()
    m3_m1 = m3_m1.reset_index()


    return r1_m1,r1_l1,m1_l1,r2_l2,m2_l2,r2_m2,r3_l3,m3_l3,r3_m3,r1_l3,r3_l1,m1_m3,m3_m1


def group_by_vertical_axis(event_file,hand):
    '''
    Divide Trials by vertical target stimuli position and return a Dataframe for each possible group
    input:
    event_file = Dataframe of event_file generated by [Exp.py]
                (contains reaction and presentation times etc.)
    hand = hand of interest either "r" or "l" as string
    output:
    vertical_left = DataFrame containing info on leftmost vertical Target positions
    vertical_middle = DataFrame containing info on middle vertical Target positions
    vertical_right = DataFrame containing info on rightmost vertical Target positions
    '''
    vertical_left = pd.DataFrame()
    vertical_middle = pd.DataFrame()
    vertical_right = pd.DataFrame()
    data = None

    for box_pos_id, data in event_file.groupby('box_pos_id'+'_'+hand):
        if list(box_pos_id)[0] ==  'r':
            if vertical_right.empty == False:
                vertical_right = vertical_right.append(data, ignore_index=True)
            elif vertical_right.empty:
                vertical_right = data

        elif list(box_pos_id)[0] == 'm':
            if vertical_middle.empty == False:
                vertical_middle = vertical_middle.append(data, ignore_index=True)
            elif vertical_middle.empty:
                vertical_middle = data

        elif list(box_pos_id)[0] == 'l':
            if vertical_left.empty == False:
                vertical_left = vertical_left.append(data, ignore_index=True)
            elif vertical_left.empty:
                vertical_left = data

    vertical_left = vertical_left.reset_index()
    vertical_middle = vertical_middle.reset_index()
    vertical_right = vertical_right.reset_index()

    return vertical_left,vertical_middle,vertical_right

def group_by_horizontal_axis(event_file, hand):
    '''
    Divide Trials by horizontal target stimuli position and return a Dataframe for each possible group
    input:
    event_file = Dataframe of event_file generated by [Exp.py]
                (contains reaction and presentation times etc.)
    hand = hand of interest either "r" or "l" as string
    output:
    low_boxes = DataFrame containing info on lower Target positions (pos1)
    middle_boxes = DataFrame containing info on middle Target positions (pos2)
    high_boxes = DataFrame containing info on upper Target positions (pos3)
    '''
    low_boxes = pd.DataFrame()
    middle_boxes = pd.DataFrame()
    high_boxes = pd.DataFrame()

    for idx, i in enumerate(event_file['box_pos_id'+'_'+hand]):
        if list(event_file['box_pos_id'+'_'+hand][idx])[1] == str(1):
#             print(list(event_file[box_pos_hand][idx]))
            if low_boxes.empty == False:
                low_boxes = low_boxes.append(event_file.loc[idx])
            elif low_boxes.empty:
                low_boxes = pd.DataFrame(event_file.loc[idx]).T
        if list(event_file['box_pos_id'+'_'+hand][idx])[1] == str(2):
            if middle_boxes.empty == False:
                middle_boxes = middle_boxes.append(event_file.loc[idx])
            elif middle_boxes.empty:
                middle_boxes = pd.DataFrame(event_file.loc[idx]).T
        if list(event_file['box_pos_id'+'_'+hand][idx])[1] == str(3):
            if high_boxes.empty == False:
                high_boxes = high_boxes.append(event_file.loc[idx])
            elif high_boxes.empty:
                high_boxes = pd.DataFrame(event_file.loc[idx]).T

    low_boxes = low_boxes.reset_index()
    middle_boxes = middle_boxes.reset_index()
    high_boxes = high_boxes.reset_index()

    return low_boxes,middle_boxes,high_boxes


def group_by_size_and_pos_2H(event_file, hand):
    '''
    Divide Trials by position and size of target stimuli and return a Dataframe
    for each possible position and size
    input:
    event_file = Dataframe of event_file generated by [Exp.py]
                (contains reaction and presentation times etc.)
    hand = hand of interest either "r" or "l" as string
    output:

    DataFrames containing info on smallest Target Cube (15cm)
    s1_r1 = DataFrame containing info on right hand Target position at approx. head height
    s1_m1 = DataFrame containing info on middle Target position at approx. head height
    s1_l1 = DataFrame containing info on left hand Target position at approx. head height

    s1_r2 = DataFrame containing info on right hand Target position at approx. shoulder height
    s1_m2 = DataFrame containing info on middle Target position at approx. shoulder height
    s1_l2 = DataFrame containing info on left hand Target position at approx. shoulder height

    s1_r3 = DataFrame containing info on right hand Target position at approx. hip height
    s1_m3 = DataFrame containing info on middle Target position at approx. hip height
    s1_l3 = DataFrame containing info on left hand Target position at approx. hip height

    DataFrames containing info on middle sized Target Cube (20cm)
    s2_r1 = DataFrame containing info on right hand Target position at approx. head height
    s2_m1 = DataFrame containing info on middle Target position at approx. head height
    s2_l1 = DataFrame containing info on left hand Target position at approx. head height

    s2_r2 = DataFrame containing info on right hand Target position at approx. shoulder height
    s2_m2 = DataFrame containing info on middle Target position at approx. shoulder height
    s2_l2 = DataFrame containing info on left hand Target position at approx. shoulder height

    s2_r3 = DataFrame containing info on right hand Target position at approx. hip height
    s2_m3 = DataFrame containing info on middle Target position at approx. hip height
    s2_l3 = DataFrame containing info on left hand Target position at approx. hip height

    DataFrames containing info on largest Target Cube (25cm)
    s3_r1 = DataFrame containing info on right hand Target position at approx. head height
    s3_m1 = DataFrame containing info on middle Target position at approx. head height
    s3_l1 = DataFrame containing info on left hand Target position at approx. head height

    s3_r2 = DataFrame containing info on right hand Target position at approx. shoulder height
    s3_m2 = DataFrame containing info on middle Target position at approx. shoulder height
    s3_l2 = DataFrame containing info on left hand Target position at approx. shoulder height

    s3_r3 = DataFrame containing info on right hand Target position at approx. hip height
    s3_m3 = DataFrame containing info on middle Target position at approx. hip height
    s3_l3 = DataFrame containing info on left hand Target position at approx. hip height
    '''
    # grouping one_hand_task: size & pos
    s1_r1 = pd.DataFrame()
    s1_m1 = pd.DataFrame()
    s1_l1 = pd.DataFrame()

    s1_r2 = pd.DataFrame()
    s1_m2 = pd.DataFrame()
    s1_l2 = pd.DataFrame()

    s1_r3 = pd.DataFrame()
    s1_m3 = pd.DataFrame()
    s1_l3 = pd.DataFrame()

    s2_r1 = pd.DataFrame()
    s2_m1 = pd.DataFrame()
    s2_l1 = pd.DataFrame()

    s2_r2 = pd.DataFrame()
    s2_m2 = pd.DataFrame()
    s2_l2 = pd.DataFrame()

    s2_r3 = pd.DataFrame()
    s2_m3 = pd.DataFrame()
    s2_l3 = pd.DataFrame()

    s3_r1 = pd.DataFrame()
    s3_m1 = pd.DataFrame()
    s3_l1 = pd.DataFrame()

    s3_r2 = pd.DataFrame()
    s3_m2 = pd.DataFrame()
    s3_l2 = pd.DataFrame()

    s3_r3 = pd.DataFrame()
    s3_m3 = pd.DataFrame()
    s3_l3 = pd.DataFrame()

    data = None

    for size_id, data in event_file.groupby('box_size_id'):
        if size_id == 's1':
            for box_pos_id, data_ in data.groupby('box_pos_id'+'_'+hand):
                print(box_pos_id)
                if box_pos_id ==  'r1':
                    if s1_r1.empty:
                        s1_r1 = data_
                    elif s1_r1.empty == False:
                        s1_r1 = s1_r1.append(data_)

                elif box_pos_id == 'm1':
                    if s1_m1.empty:
                        s1_m1 = data_
                    elif s1_m1.empty == False:
                        s1_m1 =  s1_m1.append(data_)

                elif box_pos_id == 'l1':
                    if s1_l1.empty:
                        s1_l1 = data_
                    elif s1_l1.empty == False:
                        s1_l1 = s1_l1.append(data_)

                elif box_pos_id == 'r2':
                    if s1_r2.empty:
                        s1_r2 = data_
                    elif s1_r2.empty == False:
                        s1_r2 = s1_r2.append(data_)

                elif box_pos_id == 'm2':
                    if s1_m2.empty:
                        s1_m2 = data_
                    elif s1_m2.empty == False:
                        s1_m2 = s1_m2.append(data_)

                elif box_pos_id == 'l2':
                    if s1_l2.empty:
                        s1_l2 = data_
                    elif s1_l2.empty == False:
                        s1_l2 = s1_l2.append(data_)

                elif box_pos_id == 'r3':
                    if s1_r3.empty:
                        s1_r3 = data_
                    elif s1_r3.empty == False:
                        s1_r3 = s1_r3.append(data_)

                elif box_pos_id == 'm3':
                    if s1_m3.empty:
                        s1_m3 = data_
                    elif s1_m3.empty == False:
                        s1_m3 = s1_m3.append(data_)

                elif box_pos_id == 'l3':
                    if s1_l3.empty:
                        s1_l3 = data_
                    elif s1_l3.empty == False:
                        s1_l3 = s1_l3.append(data_)

        if size_id == 's2':
            for box_pos_id, data_ in data.groupby(box_pos_hand):
                if box_pos_id ==  'r1':
                    if s2_r1.empty:
                        s2_r1 = data_
                    elif s2_r1.empty == False:
                        s2_r1 = s2_r1.append(data_)

                elif box_pos_id == 'm1':
                    if s2_m1.empty:
                        s2_m1 = data_
                    elif s2_m1.empty == False:
                        s2_m1 = s2_m1.append(data_)

                elif box_pos_id == 'l1':
                    if s2_l1.empty:
                        s2_l1 = data_
                    elif s2_l1.empty == False:
                        s2_l1 = s2_l1.append(data_)

                elif box_pos_id == 'r2':
                    if s2_r2.empty:
                        s2_r2 = data_
                    elif s2_r2.empty == False:
                        s2_r2 = s2_r2.append(data_)

                elif box_pos_id == 'm2':
                    if s2_m2.empty:
                        s2_m2 = data_
                    elif s2_m2.empty == False:
                        s2_m2 = s2_m2.append(data_)

                elif box_pos_id == 'l2':
                    if s2_l2.empty:
                        s2_l2 = data_
                    elif s2_l2.empty == False:
                        s2_l2 = s2_l2.append(data_)

                elif box_pos_id == 'r3':
                    if s2_r3.empty:
                        s2_r3 = data_
                    elif s2_r3.empty == False:
                        s2_r3 = s2_r3.append(data_)

                elif box_pos_id == 'm3':
                    if s2_m3.empty:
                        s2_m3 = data_
                    elif s2_m3.empty == False:
                        s2_m3 = s2_m3.append(data_)

                elif box_pos_id == 'l3':
                    if s2_l3.empty:
                        s2_l3 = data_
                    elif s2_l3.empty == False:
                        s2_l3 = s2_l3.append(data_)

        if size_id == 's3':
            for box_pos_id, data_ in data.groupby(box_pos_hand):
                if box_pos_id ==  'r1':
                    if s3_r1.empty:
                        s3_r1 = data_
                    elif s3_r1.empty == False:
                        s3_r1 = s3_r1.append(data_)

                elif box_pos_id == 'm1':
                    if s3_m1.empty:
                        s3_m1 = data_
                    elif s3_m1.empty == False:
                        s3_m1 = s3_m1.append(data_)

                elif box_pos_id == 'l1':
                    if s3_l1.empty:
                        s3_l1 = data_
                    elif s3_l1.empty == False:
                        s3_l1 = s3_l1.append(data_)

                elif box_pos_id == 'r2':
                    if s3_r2.empty:
                        s3_r2 = data_
                    elif s3_r2.empty == False:
                        s3_r2 = s3_r2.append(data_)

                elif box_pos_id == 'm2':
                    if s3_m2.empty:
                        s3_m2 = data_
                    elif s3_m2.empty == False:
                        s3_m2 = s3_m2.append(data_)

                elif box_pos_id == 'l2':
                    if s3_l2.empty:
                        s3_l2 = data_
                    elif s3_l2.empty == False:
                        s3_l2 = s3_l2.append(data_)

                elif box_pos_id == 'r3':
                    if s3_r3.empty:
                        s3_r3 = data_
                    elif s3_r3.empty == False:
                        s3_r3 = s3_r3.append(data_)

                elif box_pos_id == 'm3':

                    if s3_m3.empty:
                        s3_m3 = data_
                    elif s3_m3.empty == False:
                        s3_m3 = s3_m3.append(data_)

                elif box_pos_id == 'l3':
                    if s3_l3.empty:
                        s3_l3 = data_
                    elif s3_l3.empty == False:
                        s3_l3 = s3_l3.append(data_)

    s1_r1 = s1_r1.reset_index()
    s1_m1 = s1_m1.reset_index()
    s1_l1 = s1_l1.reset_index()

    s1_r2 = s1_r2.reset_index()
    s1_m2 = s1_m2.reset_index()
    s1_l2 = s1_l2.reset_index()

    s1_r3 = s1_r3.reset_index()
    s1_m3 = s1_m3.reset_index()
    s1_l3 = s1_l3.reset_index()

    s2_r1 = s2_r1.reset_index()
    s2_m1 = s2_m1.reset_index()
    s2_l1 = s2_l1.reset_index()

    s2_r2 = s2_r2.reset_index()
    s2_m2 = s2_m2.reset_index()
    s2_l2 = s2_l2.reset_index()

    s2_r3 = s2_r3.reset_index()
    s2_m3 = s2_m3.reset_index()
    s2_l3 = s2_l3.reset_index()


    s3_r1 = s3_r1.reset_index()
    s3_m1 = s3_m1.reset_index()
    s3_l1 = s3_l1.reset_index()

    s3_r2 = s3_r2.reset_index()
    s3_m2 = s3_m2.reset_index()
    s3_l2 = s3_l2.reset_index()

    s3_r3 = s3_r3.reset_index()
    s3_m3 = s3_m3.reset_index()
    s3_l3 = s3_l3.reset_index()

    return s1_r1,s1_r2,s1_r3,s1_m1, s1_m2,s1_m3, s1_l1,s1_l2,s1_l3, s2_r1,s2_r2,s2_r3,s2_m1,s2_m2,s2_m3, s2_l1,s2_l2,s2_l3,s3_r1,s3_r2,s3_r3,s3_m1, s3_m2,s3_m3, s3_l1,s3_l2,s3_l3


def mean_trial_length(event_file, data_file, markers, hand):
    '''
    returns mean length of trials in event file
    
    input:
    event_file: file containing start and end time of trials of interest
    data_file: file containg positional data for each marker in 3-dimennsions (x,y,z)
                and time at which position was sampled
    markers: list of markers (numbered 1-37) of interest

    output:
	returns mean length of trials in event file
    '''
    start = []
    entered = []
    exited = []
    lens = []

    for i in event_file['trial_start']:
        start.append(i)
    for i in event_file['entered_'+hand]:
        entered.append(i)
    for i in event_file['exited_'+hand]:
        exited.append(i)

    for idx, i in enumerate(start):
        index_start = abs(data_file['time'] - start[idx]).idxmin()
        index_end = abs(data_file['time'] - (entered[idx])).idxmin()

        for j in data_file[markers]:
            if j.split('_')[2] == 'x':
                arr = np.array(data_file[j].loc[index_start:index_end])
                arr = arr.flatten()
                len_arr = len(arr)
                #print(len_arr)
                lens.append(len_arr)
                
    len_array = np.array(lens)
    print(len_array.mean())

# def extract_and_resample(event_file, data_file):
def extract_and_resample_2H(event_file, data_file, markers, hand):
    '''
    Extract and Trials specified in event_file for specified marker and hand from data_file.
    Resample extracted trials to commmon length, while correcting for ringing artefacts.

    input:
    event_file: file containing start and end time of trials of interest
    data_file: file containg positional data for each marker in 3-dimennsions (x,y,z)
                and time at which position was sampled
    markers: list of markers (numbered 1-37) of interest
    hand = hand of interest either "r" or "l" as string

    output:
    df_x = Dataframe containing trials as columns, samples as rows
            positional data in x dimension in cells
    df_y = Dataframe containing trials as columns, samples as rows
            positional data in y dimension in cells
    df_z = Dataframe containing trials as columns, samples as rows
            positional data in z dimension in cells
    '''
    start = []
    entered = []
    exited = []
    df_x = pd.DataFrame()
    df_y = pd.DataFrame()
    df_z = pd.DataFrame()

    for i in event_file['trial_start']:
        start.append(i+0)

    for i in event_file['entered'+'_'+hand]:
        entered.append(i+1)
    for i in event_file['exited'+'_'+hand]:
        exited.append(i)

    for idx, i in enumerate(start):
	# extract start point
        index_start = abs(data_file['time'] - start[idx]).idxmin()
        index_end = abs(data_file['time'] - exited[idx]).idxmin()

        for j in data_file[markers]:
            if j.split('_')[2] == 'x':
                arr = np.array(data_file[j].loc[index_start:index_end])
                arr = arr.flatten()
                # resample
                pad = np.pad(arr, mode='edge',pad_width=(len(arr)))
                resampled = signal.resample(pad, 900)
                excess_start = len(resampled)//3
                excess_end = (len(resampled)//3)*2
                resampled = resampled[excess_start:excess_end]
                df_x['trial_'+str(idx)+'_'+ str(j)] = resampled
            elif j.split('_')[2] == 'y':
                arr = np.array(data_file[j].loc[index_start:index_end])
                arr = arr.flatten()
                # resample
                pad = np.pad(arr, mode='edge',pad_width=(len(arr)))
                resampled = signal.resample(pad, 900)
                excess_start = len(resampled)//3
                excess_end = (len(resampled)//3)*2
                resampled = resampled[excess_start:excess_end]
                df_y['trial_'+str(idx)+'_'+ str(j)] = resampled
            elif j.split('_')[2] == 'z':
                arr = np.array(data_file[j].loc[index_start:index_end])
                arr = arr.flatten()
                # resample
                pad = np.pad(arr, mode='edge',pad_width=(len(arr)))
                resampled = signal.resample(pad, 900)
                excess_start = len(resampled)//3
                excess_end = (len(resampled)//3)*2
                resampled = resampled[excess_start:excess_end]
                df_z['trial_'+str(idx)+'_'+ str(j)] = resampled
    return df_x, df_y, df_z


def get_corrected(dimension, df):
    '''
    Extract positional mean and variance of trials
    zero-center trials by substracting mean and return
    mean, variance and corrected trials as Dataframe

    input:
    dimension: dimension of interest, either 'x', 'y' or 'z' as string
    df = Dataframe generated by extract_and_resample_2H function
         containing trials as columns, samples as rows
         positional data in x,y or z dimension in cells
    output:
    mean_dimension: matrix containing mean for every sample point
                    over every trial in df
    variance_dimension: matrix containing variance for every sample point
                    over every trial in df
    df_corrected = Dataframeand containing trials as columns, samples as rows
         and zero-centered positional data in x,y or z dimension in cells
    '''

    mean_dimension = []
    variance_dimension = []
    corrected_mean = []
    # concatenate all trials of specified dimension (x,y or z) as columns into single DataFrame
    #  calculate mean value for every timepoint
    for i in range(len(df)):
        mean_dimension.append(df.loc[i].mean())

    #  calculate variance for every timepoint
    for i in range(len(df)):
        variance_dimension.append(df.loc[i].var())

    # correct trial for mean movement
    counter_x = 0
    corrected_mean = []

    for i in (df):
        corrected_mean = []
        for j in range(len(df[i])):
            corrected_mean.append(df[i][j] - mean_dimension[j])
        if counter_x == 0:
            df_corrected = pd.DataFrame({str(df.columns[counter_x])+'_corrected':corrected_mean})
        elif counter_x >0:
            df_corrected[str(df.columns[counter_x])+'_corrected'] = corrected_mean
        counter_x+=1

    return mean_dimension, variance_dimension, df_corrected

# claculate velocity of resampled signal
def plot_velocity(movement_data):
    velocity = np.diff(movement_data)/np.diff(range(len(movement_data)))
    fig = plt.figure()
    plt.plot(range(299),velocity)
    return velocity

def plot_variance(var_x,var_y,var_z, mean_x, mean_y, mean_z):
    sns.set_context("notebook", font_scale=1.5, rc={"lines.linewidth": 2.5})
    sns.set_style('whitegrid')
    sns.set_palette('colorblind')
    fig, ax = plt.subplots(nrows=2, ncols=3, figsize=(35, 20))

    ax[0][0].plot(range(len(var_x)), var_x, marker='o',color='crimson')
    ax[0][0].set_title('variance_x',fontsize=20)
    ax[0][0].set_xlim()
    ax[0][0].set_ylim(0,0.15)
    ax[0][0].set_xlabel('samples',fontsize=20)
    ax[0][0].set_ylabel('x_dim variance',fontsize=20)
    ax[0][0].tick_params(labelsize=20)


    ax[1][0].plot(range(len(mean_x)), mean_x, marker='o', color='maroon')
    ax[1][0].set_title('mean_x',fontsize=20)
    ax[1][0].set_ylim(0,2)
    ax[1][0].set_xlabel('samples',fontsize=20)
    ax[1][0].set_ylabel('x_dim (meter)',fontsize=20)
    ax[1][0].tick_params(labelsize=20)



    ax[0][1].plot(range(len(var_y)), var_y, marker='o',color='crimson')
    ax[0][1].set_title('variance_y',fontsize=20)
    ax[0][1].set_ylim(0,0.15)
    ax[0][1].set_xlabel('samples',fontsize=20)
    ax[0][1].set_ylabel('y_dim variance',fontsize=20)
    ax[0][1].tick_params(labelsize=20)


    ax[1][1].plot(range(len(mean_y)), mean_y, marker='o', color='maroon')
    ax[1][1].set_title('mean_y',fontsize=20)
    ax[1][1].set_ylim(0,2)
    ax[1][1].set_xlabel('samples',fontsize=20)
    ax[1][1].set_ylabel('y_dim (meter)',fontsize=20)
    ax[1][1].tick_params(labelsize=20)


    ax[0][2].plot(range(len(var_z)), var_z, marker='o',color='crimson')
    ax[0][2].set_title('variance_z',fontsize=20)
    ax[0][2].set_ylim(0,0.15)
    ax[0][2].set_xlabel('samples',fontsize=20)
    ax[0][2].set_ylabel('z_dim variance',fontsize=20)
    ax[0][2].tick_params(labelsize=20)




    ax[1][2].plot(range(len(mean_z)), mean_z, marker='o', color='maroon')
    ax[1][2].set_title('mean_z',fontsize=20)
    ax[1][2].set_ylim(0,2)
    ax[1][2].set_xlabel('samples',fontsize=20)
    ax[1][2].set_ylabel('z_dim (meter)',fontsize=20)
    ax[1][2].tick_params(labelsize=20)

    sns.despine(offset=20);
    plt.show()
    return fig, ax

def reshape(df_x, df_y, df_z):
    '''
    Concatenate and reshape Dataframes generated by get_corrected function for SVD.

    input:
    df_x = zero-centered positional data x-dim
    df_y = zero-centered positional data y-dim
    df_z = zero-centered positional data z-dim

    output:
    transformed_data = Dataframe with x,y, z dimensions as columns and
    sample*trials as rows
    '''
    rows = []

    for idx, i in enumerate(df_x.columns):
        for j in range(len(df_x)):
            rows.append([df_x[df_x.columns[idx]][j], df_y[df_y.columns[idx]][j],df_z[df_z.columns[idx]][j]])

    array = np.asarray(rows)

    transformed_data = pd.DataFrame({'x':array[:, 0],'y':array[:, 1],'z':array[:, 2]})
    return transformed_data

## SVD
def extract_mean_factor_loading(reshaped):
    '''
    Perform SVD on Data generated by "reshaped"-function extract mean loading
    and standard deviation of mean of first singular vector of U-matrix.
    Concatenate mean_loadings for each trial into single column matrix.

    input:
    reshaped: Dataframe with x,y, z dimensions as columns and
    sample*trials as rows

    output:
    U = n*n matrux containing left singular vectors
    S = m*m diagonal matrix containing singular values
    VT = m*m matrix containing right singular vectors
    mean_loadings = mean loadings on u1 over all trials
    stdm = standard deviation of mean loadings
    concatenated = dataframe containing mean loadings; trial*samples long

    '''
    U, s, VT = scipy.linalg.svd(reshaped)
    S = np.diag(s)
    len_x = []
    for i in range(1,len(s)+1):
        print(i)
        len_x.append(i)

    sns.barplot(y=s, x=len_x)
    #plt.ylim(0,4)
    plt.title('Singular Values:')
    plt.show()

    # show explained variance
    var_explained = np.round(s**2/np.sum(s**2), decimals=3)
    #print(type(var_explained))
    legend_elements = []
    for idx, i in enumerate(var_explained):
        element = Line2D([0], [0], alpha=0, lw=2, marker='o', markersize=15, label=('SV '+str(idx)+': '+str(var_explained[idx])))
        legend_elements.append(element)
    fig1 = plt.figure()
    sns.barplot(x=list(range(1,len(var_explained)+1)),
            y=var_explained)
    plt.xlabel('Singular Values', fontsize=16)
    plt.ylabel('Variance Explained (%)', fontsize=16)
    plt.ylim(0,1)
    plt.legend(handles=legend_elements, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., prop={'size': 10})
    plt.show()

    # reshape to easier calculate mean loadings
    counter = 0
    trial_length = 300
    concatenated = pd.DataFrame(columns=range(len(reshaped)//trial_length))
    for i in range(len(reshaped)//trial_length):

        counter +=trial_length
        if i == 0:
            concatenated[concatenated.columns[i]] = U[:counter,0]
        elif i >0:
            concatenated[concatenated.columns[i]] = U[counter-trial_length:counter,0]

    # transform trial to matrix and transform each point to absolute value
    concatenated = concatenated.values
    concatenated = np.absolute(concatenated)

    # extract mean loading and standarad error of mean
    mean_loadings = []
    stdm = []
    for i in range(trial_length):
        mean_loadings.append(concatenated[i].mean())
        stdm.append(scipy.stats.sem(concatenated[i]))

    mean_loadings = np.array(mean_loadings)
    stdm = np.array(stdm)
    return U, s, S, VT, mean_loadings, stdm, concatenated, var_explained

def plot_mean_loadings(mean_loadings):
    '''
    Plot mean loadings generated by extract_mean_factor_loading function
    and return figure
    '''
    # plot mean loadings
    sns.set_context("notebook", font_scale=1.5, rc={"lines.linewidth": 2.5})
    sns.set_style('whitegrid')
    sns.set_palette('colorblind')

    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(14, 8))

    ax.plot(range(len(mean_loadings)), mean_loadings, marker='o',color='crimson')
    ax.set_xlabel('samples')
    ax.set_ylabel('mean_loadings')
#    ax.set_ylim(-1,1)
    plt.show()
    return fig

def plot_stdm(stdm):
    '''
    Plot standard deviation of mean generated by extract_mean_factor_loading function
    and return figure
    '''
    # plot std of m
    sns.set_context("notebook", font_scale=1.5, rc={"lines.linewidth": 2.5})
    sns.set_style('whitegrid')
    sns.set_palette('colorblind')

    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(14, 8))

    ax.plot(range(len(stdm)), stdm, marker='o',color='royalblue')
    ax.set_xlabel('samples')
    ax.set_ylabel('standard error of mean')
    ax.set_ylim(-0.2,0.2)
    plt.show()
    return fig

## Actual analysis and stuff

def analysis_by_pos(event_file,data_file,marker, pos_of_interest, hand, path):
    '''
    Function to analyse movement data based on position of target.
    Links functions
        - group_by_position_2H - group data based on position
        - get corrected - zero-center positional data
        - reshape - transform matrix to necessary format for svd
        - extract_mean_factor_loading - perform svd and extract mean factor loading of u1
        - plot_mean_loadings - plot mean factor loading

    input:
    event_file: file containing specified events and their time stamps
    datafile: containing positional data of markers in x,y,z dimensions
    marker = name of marker of interest as list of strings (marker-names in data_file.columns)
    pos_of_interest = name of target position of interest as list of strings (e.g. r1, m2 etc.)
    hand = hand of interest either "r" or "l" as string
    path = path to save data to

    output:
    mean_trajectories = csv file containing mean trajectory of marker for target position
    U_df = csv file (n*n matrix) containing left singular vectors
    S_df = csv file (m*m diagonal matrix) containing singular values
    VT_df = csv file (m*m matrix) containing right singular vectors
    ML_df = csv file of mean loadings on u1 over all trials
    stdm_df = csv file of standard deviation of mean loadings
    reshaped_df = Dataframe with x,y, z dimensions as columns and
    sample*trials as rows

    '''
    # create subj directory
    path = path + (marker[0][0:9])+'/' + hand + '_' + (marker[0][0:9])+'_'
    try:
        os.makedirs(path)
    except OSError as e:
        if e.errno != errno.EEXIST:
            raise
    r1,r2,r3,m1,m2,m3,l1,l2,l3 = group_by_position_2H(event_file, hand)

    for idx, i in enumerate(pos_of_interest):
#         print(i)
        try:
            if i == 'r1':
                print('+++++++++++++++++++++++++++++   Analysing:  '+i +'   +++++++++++++++++++++++++++++++++++++')
                df_x, df_y, df_z = extract_and_resample_2H(r1, data_file, marker, hand)
                x_mean_dimension, x_variance_dimension, x_df_corrected = get_corrected('x', df_x)
                y_mean_dimension, y_variance_dimension, y_df_corrected = get_corrected('y', df_y)
                z_mean_dimension, z_variance_dimension, z_df_corrected = get_corrected('z', df_z)
                # to csv
                reshaped_df = reshape(x_df_corrected, y_df_corrected, z_df_corrected)
                U, s, S, VT, mean_loadings, stdm, concatenated, var_explained = extract_mean_factor_loading(reshaped_df)
                # to csv
                fig = plot_mean_loadings(mean_loadings)
            elif i == 'r2':
                print('+++++++++++++++++++++++++++++   Analysing:  '+i +'   +++++++++++++++++++++++++++++++++++++')
                df_x, df_y, df_z = extract_and_resample_2H(r2, data_file, marker, hand)
                x_mean_dimension, x_variance_dimension, x_df_corrected = get_corrected('x', df_x)
                y_mean_dimension, y_variance_dimension, y_df_corrected = get_corrected('y', df_y)
                z_mean_dimension, z_variance_dimension, z_df_corrected = get_corrected('z', df_z)
                # to csv
                reshaped_df = reshape(x_df_corrected, y_df_corrected, z_df_corrected)
                U, s, S, VT, mean_loadings, stdm, concatenated, var_explained = extract_mean_factor_loading(reshaped_df)
                # to csv
                fig = plot_mean_loadings(mean_loadings)
            elif i == 'r3':
                print('+++++++++++++++++++++++++++++   Analysing:  '+i +'   +++++++++++++++++++++++++++++++++++++')
                df_x, df_y, df_z = extract_and_resample_2H(r3, data_file, marker, hand)
                x_mean_dimension, x_variance_dimension, x_df_corrected = get_corrected('x', df_x)
                y_mean_dimension, y_variance_dimension, y_df_corrected = get_corrected('y', df_y)
                z_mean_dimension, z_variance_dimension, z_df_corrected = get_corrected('z', df_z)
                # to csv
                reshaped_df = reshape(x_df_corrected, y_df_corrected, z_df_corrected)
                U, s, S, VT, mean_loadings, stdm, concatenated, var_explained = extract_mean_factor_loading(reshaped_df)
                # to csv
                plot_mean_loadings(mean_loadings)
            elif i == 'm1':
                print('+++++++++++++++++++++++++++++   Analysing:  '+i +'   +++++++++++++++++++++++++++++++++++++')
                df_x, df_y, df_z = extract_and_resample_2H(m1, data_file, marker, hand)
                x_mean_dimension, x_variance_dimension, x_df_corrected = get_corrected('x', df_x)
                y_mean_dimension, y_variance_dimension, y_df_corrected = get_corrected('y', df_y)
                z_mean_dimension, z_variance_dimension, z_df_corrected = get_corrected('z', df_z)
                # to csv
                reshaped_df = reshape(x_df_corrected, y_df_corrected, z_df_corrected)
                U, s, S, VT, mean_loadings, stdm, concatenated, var_explained = extract_mean_factor_loading(reshaped_df)
                # to csv
                fig = plot_mean_loadings(mean_loadings)
            elif i == 'm2':
                print('+++++++++++++++++++++   Analysing:  '+i +'   +++++++++++++++++++++++++++++++++++++++')
                df_x, df_y, df_z = extract_and_resample_2H(m2, data_file, marker, hand)
                x_mean_dimension, x_variance_dimension, x_df_corrected = get_corrected('x', df_x)
                y_mean_dimension, y_variance_dimension, y_df_corrected = get_corrected('y', df_y)
                z_mean_dimension, z_variance_dimension, z_df_corrected = get_corrected('z', df_z)
                # to csv
                reshaped_df = reshape(x_df_corrected, y_df_corrected, z_df_corrected)
                U, s, S, VT, mean_loadings, stdm, concatenated, var_explained = extract_mean_factor_loading(reshaped_df)
                # to csv
                fig = plot_mean_loadings(mean_loadings)
            elif i == 'm3':
                print('+++++++++++++++++++++++++++++   Analysing:  '+i +'   +++++++++++++++++++++++++++++++++++++')
                df_x, df_y, df_z = extract_and_resample_2H(m3, data_file, marker, hand)
                x_mean_dimension, x_variance_dimension, x_df_corrected = get_corrected('x', df_x)
                y_mean_dimension, y_variance_dimension, y_df_corrected = get_corrected('y', df_y)
                z_mean_dimension, z_variance_dimension, z_df_corrected = get_corrected('z', df_z)
                # to csv
                reshaped_df = reshape(x_df_corrected, y_df_corrected, z_df_corrected)
                U, s, S, VT, mean_loadings, stdm, concatenated, var_explained = extract_mean_factor_loading(reshaped_df)
                # to csv
                fig = plot_mean_loadings(mean_loadings)
            elif i == 'l1':
                print('+++++++++++++++++++++++++++++   Analysing:  '+i +'   +++++++++++++++++++++++++++++++++++++')
                df_x, df_y, df_z = extract_and_resample_2H(l1, data_file, marker, hand)
                x_mean_dimension, x_variance_dimension, x_df_corrected = get_corrected('x', df_x)
                y_mean_dimension, y_variance_dimension, y_df_corrected = get_corrected('y', df_y)
                z_mean_dimension, z_variance_dimension, z_df_corrected = get_corrected('z', df_z)
                # to csv
                reshaped_df = reshape(x_df_corrected, y_df_corrected, z_df_corrected)
                U, s, S, VT, mean_loadings, stdm, concatenated, var_explained = extract_mean_factor_loading(reshaped_df)
                # to csv
                fig = plot_mean_loadings(mean_loadings)
            elif i == 'l2':
                print('+++++++++++++++++++++++++++++   Analysing:  '+i +'   +++++++++++++++++++++++++++++++++++++')
                df_x, df_y, df_z = extract_and_resample_2H(l2, data_file, marker, hand)
                x_mean_dimension, x_variance_dimension, x_df_corrected = get_corrected('x', df_x)
                y_mean_dimension, y_variance_dimension, y_df_corrected = get_corrected('y', df_y)
                z_mean_dimension, z_variance_dimension, z_df_corrected = get_corrected('z', df_z)
                # to csv
                reshaped_df = reshape(x_df_corrected, y_df_corrected, z_df_corrected)
                U, s, S, VT, mean_loadings, stdm, concatenated, var_explained = extract_mean_factor_loading(reshaped_df)
                # to csv
                fig = plot_mean_loadings(mean_loadings)
            elif i == 'l3':
                print('+++++++++++++++++++++++++++++   Analysing:  '+i +'   +++++++++++++++++++++++++++++++++++++')
                df_x, df_y, df_z = extract_and_resample_2H(l3, data_file, marker, hand)
                x_mean_dimension, x_variance_dimension, x_df_corrected = get_corrected('x', df_x)
                y_mean_dimension, y_variance_dimension, y_df_corrected = get_corrected('y', df_y)
                z_mean_dimension, z_variance_dimension, z_df_corrected = get_corrected('z', df_z)
                # to csv
                reshaped_df = reshape(x_df_corrected, y_df_corrected, z_df_corrected)
                U, s, S, VT, mean_loadings, stdm, concatenated, var_explained = extract_mean_factor_loading(reshaped_df)
                # to csv
                fig = plot_mean_loadings(mean_loadings)
            df_x.to_csv(path+i+'_x_dim'+'.csv', sep=',', index=False)
            df_y.to_csv(path+i+'_y_dim'+'.csv', sep=',',  index=False)
            df_z.to_csv(path+i+'_trials_z_dim'+'.csv', sep=',', index=False)
            mean_trajectories = pd.DataFrame({'x_mean': x_mean_dimension,'y_mean':y_mean_dimension,
                                             'z_mean': z_mean_dimension, 'x_var':x_variance_dimension,
                                             'y_var':y_variance_dimension,'z_var':z_variance_dimension})
            mean_trajectories.to_csv(path+i+'_mean_trajectories'+'.csv', sep=',', index=False)
            #U_df = pd.DataFrame(data=U.astype(float))
            #U_df.to_csv(path+i+'_U'+'.csv', sep=',', header=False, float_format='%.2f', index=False)
            s_df = pd.DataFrame(s.astype(float))
            s_df.to_csv(path+i+'_s'+'.csv', sep=',', header=False, index=False)
            S_df = pd.DataFrame(S.astype(float))
            S_df.to_csv(path+i+'_S'+'.csv', sep=',', header=False, index=False)
            VT_df = pd.DataFrame(VT.astype(float))
            VT_df.to_csv(path+i+'_VT'+'.csv', sep=',', header=False, index=False)
            ML_df = pd.DataFrame(mean_loadings.astype(float))
            ML_df.to_csv(path+i+'_mean_loadings'+'.csv', sep=',', header=False,index=False)
            stdm_df = pd.DataFrame(stdm.astype(float))
            stdm_df.to_csv(path+i+'_stdm'+'.csv', sep=',', header=False, index=False)
            var_expl_df = pd.DataFrame(var_explained.astype(float))
            var_expl_df.to_csv(path+i+'_stdm'+'.csv', sep=',', header=False, index=False)
            reshaped_df = pd.DataFrame(reshaped_df.astype(float))
            reshaped_df.to_csv(path+i+'_corrected_trials-xyz'+'.csv', sep=',', index=False)
            return ML_df
        except KeyError:
            print("""passed: {pos_of_interest} -> check dataframe!""".format(pos_of_interest=pos_of_interest[idx]))

def analysis_by_result(event_file,data_file,marker,result_group, pos_of_interest, hand, path):
    '''
    Function to analyse movement data based on result of trial.
    Links functions
        - group_by_result_2H - group data based on results
        - get corrected - zero-center positional data
        - reshape - transform matrix to necessary format for svd
        - extract_mean_factor_loading - perform svd and extract mean factor loading of u1
        - plot_mean_loadings - plot mean factor loading

    input:
    event_file: file containing specified events and their time stamps
    datafile: containing positional data of markers in x,y,z dimensions
    marker = name of marker of interest as list of strings (marker-names in data_file.columns)
    result_group = name of result of interest as string ("successful" or "failed")
    hand = hand of interest either "r" or "l" as string
    path = path to save data to

    output:
    mean_trajectories = csv file containing mean trajectory of marker for target position
    U_df = csv file (n*n matrix) containing left singular vectors
    S_df = csv file (m*m diagonal matrix) containing singular values
    VT_df = csv file (m*m matrix) containing right singular vectors
    ML_df = csv file of mean loadings on u1 over all trials
    stdm_df = csv file of standard deviation of mean loadings
    reshaped_df = Dataframe with x,y, z dimensions as columns and
    sample*trials as rows

    '''
    path = path + '2H/'+hand +'/'+result_group+'/pos/' + (marker[0][0:9])+'/'
    try:
        os.makedirs(path)
    except OSError as e:
        if e.errno != errno.EEXIST:
            raise

    successful_trials, failed_trials = group_by_result_2H(event_file, hand)

    if result_group == 'successful':
        print(result_group)
        r1,r2,r3,m1,m2,m3,l1,l2,l3 = group_by_position_2H(successful_trials, hand)
    elif result_group == 'failed':
        print(result_group)
        r1,r2,r3,m1,m2,m3,l1,l2,l3 = group_by_position_2H(failed_trials, hand)

    for idx, i in enumerate(pos_of_interest):
#         print(i)
        try:
            if i == 'r1':
                df_x, df_y, df_z = extract_and_resample_2H(r1, data_file, marker, hand)
                x_mean_dimension, x_variance_dimension, x_df_corrected = get_corrected('x', df_x)
                y_mean_dimension, y_variance_dimension, y_df_corrected = get_corrected('y', df_y)
                z_mean_dimension, z_variance_dimension, z_df_corrected = get_corrected('z', df_z)
                # to csv
                reshaped_df = reshape(x_df_corrected, y_df_corrected, z_df_corrected)
                U, s, S, VT, mean_loadings, stdm, concatenated, var_explained = extract_mean_factor_loading(reshaped_df)
                # to csv
                fig = plot_mean_loadings(mean_loadings)
            elif i == 'r2':
                df_x, df_y, df_z = extract_and_resample_2H(r2, data_file, marker, hand)
                x_mean_dimension, x_variance_dimension, x_df_corrected = get_corrected('x', df_x)
                y_mean_dimension, y_variance_dimension, y_df_corrected = get_corrected('y', df_y)
                z_mean_dimension, z_variance_dimension, z_df_corrected = get_corrected('z', df_z)
                # to csv
                reshaped_df = reshape(x_df_corrected, y_df_corrected, z_df_corrected)
                U, s, S, VT, mean_loadings, stdm, concatenated, var_explained = extract_mean_factor_loading(reshaped_df)
                # to csv
                fig = plot_mean_loadings(mean_loadings)
            elif i == 'r3':
                df_x, df_y, df_z = extract_and_resample_2H(r3, data_file, marker, hand)
                x_mean_dimension, x_variance_dimension, x_df_corrected = get_corrected('x', df_x)
                y_mean_dimension, y_variance_dimension, y_df_corrected = get_corrected('y', df_y)
                z_mean_dimension, z_variance_dimension, z_df_corrected = get_corrected('z', df_z)
                # to csv
                reshaped_df = reshape(x_df_corrected, y_df_corrected, z_df_corrected)
                U, s, S, VT, mean_loadings, stdm, concatenated, var_explained = extract_mean_factor_loading(reshaped_df)
                # to csv
                fig = plot_mean_loadings(mean_loadings)
            elif i == 'm1':
                df_x, df_y, df_z = extract_and_resample_2H(m1, data_file, marker, hand)
                x_mean_dimension, x_variance_dimension, x_df_corrected = get_corrected('x', df_x)
                y_mean_dimension, y_variance_dimension, y_df_corrected = get_corrected('y', df_y)
                z_mean_dimension, z_variance_dimension, z_df_corrected = get_corrected('z', df_z)
                # to csv
                reshaped_df = reshape(x_df_corrected, y_df_corrected, z_df_corrected)
                U, s, S, VT, mean_loadings, stdm, concatenated, var_explained = extract_mean_factor_loading(reshaped_df)
                # to csv
                fig = plot_mean_loadings(mean_loadings)
            elif i == 'm2':
                df_x, df_y, df_z = extract_and_resample_2H(m2, data_file, marker, hand)
                x_mean_dimension, x_variance_dimension, x_df_corrected = get_corrected('x', df_x)
                y_mean_dimension, y_variance_dimension, y_df_corrected = get_corrected('y', df_y)
                z_mean_dimension, z_variance_dimension, z_df_corrected = get_corrected('z', df_z)
                # to csv
                reshaped_df = reshape(x_df_corrected, y_df_corrected, z_df_corrected)
                U, s, S, VT, mean_loadings, stdm, concatenated, var_explained = extract_mean_factor_loading(reshaped_df)
                # to csv
                plot_mean_loadings(mean_loadings)
            elif i == 'm3':
                df_x, df_y, df_z = extract_and_resample_2H(m3, data_file, marker, hand)
                x_mean_dimension, x_variance_dimension, x_df_corrected = get_corrected('x', df_x)
                y_mean_dimension, y_variance_dimension, y_df_corrected = get_corrected('y', df_y)
                z_mean_dimension, z_variance_dimension, z_df_corrected = get_corrected('z', df_z)
                # to csv
                reshaped_df = reshape(x_df_corrected, y_df_corrected, z_df_corrected)
                U, s, S, VT, mean_loadings, stdm, concatenated, var_explained = extract_mean_factor_loading(reshaped_df)
                # to csv
                fig = plot_mean_loadings(mean_loadings)
            elif i == 'l1':
                df_x, df_y, df_z = extract_and_resample_2H(l1, data_file, marker, hand)
                x_mean_dimension, x_variance_dimension, x_df_corrected = get_corrected('x', df_x)
                y_mean_dimension, y_variance_dimension, y_df_corrected = get_corrected('y', df_y)
                z_mean_dimension, z_variance_dimension, z_df_corrected = get_corrected('z', df_z)
                # to csv
                reshaped_df = reshape(x_df_corrected, y_df_corrected, z_df_corrected)
                U, s, S, VT, mean_loadings, stdm, concatenated, var_explained = extract_mean_factor_loading(reshaped_df)
                # to csv
                fig = plot_mean_loadings(mean_loadings)
            elif i == 'l2':
                df_x, df_y, df_z = extract_and_resample_2H(l2, data_file, marker, hand)
                x_mean_dimension, x_variance_dimension, x_df_corrected = get_corrected('x', df_x)
                y_mean_dimension, y_variance_dimension, y_df_corrected = get_corrected('y', df_y)
                z_mean_dimension, z_variance_dimension, z_df_corrected = get_corrected('z', df_z)
                # to csv
                reshaped_df = reshape(x_df_corrected, y_df_corrected, z_df_corrected)
                U, s, S, VT, mean_loadings, stdm, concatenated, var_explained = extract_mean_factor_loading(reshaped_df)
                # to csv
                fig = plot_mean_loadings(mean_loadings)
            elif i == 'l3':
                df_x, df_y, df_z = extract_and_resample_2H(l3, data_file, marker, hand)
                x_mean_dimension, x_variance_dimension, x_df_corrected = get_corrected('x', df_x)
                y_mean_dimension, y_variance_dimension, y_df_corrected = get_corrected('y', df_y)
                z_mean_dimension, z_variance_dimension, z_df_corrected = get_corrected('z', df_z)
                # to csv
                reshaped_df = reshape(x_df_corrected, y_df_corrected, z_df_corrected)
                U, s, S, VT, mean_loadings, stdm, concatenated, var_explained = extract_mean_factor_loading(reshaped_df)
                # to csv
                fig = plot_mean_loadings(mean_loadings)
            df_x.to_csv(path+i+'_x_dim'+'.csv', sep=',', index=False)
            df_y.to_csv(path+i+'_y_dim'+'.csv', sep=',',  index=False)
            df_z.to_csv(path+i+'_trials_z_dim'+'.csv', sep=',', index=False)
            mean_trajectories = pd.DataFrame({'x_mean': x_mean_dimension,'y_mean':y_mean_dimension,
                                             'z_mean': z_mean_dimension, 'x_var':x_variance_dimension,
                                             'y_var':y_variance_dimension,'z_var':z_variance_dimension})
            mean_trajectories.to_csv(path+i+'_mean_trajectories'+'.csv', sep=',', index=False)
            #U_df = pd.DataFrame(data=U.astype(float))
            #U_df.to_csv(path+i+'_U'+'.csv', sep=',', header=False, float_format='%.2f', index=False)
            s_df = pd.DataFrame(s.astype(float))
            s_df.to_csv(path+i+'_s'+'.csv', sep=',', header=False, index=False)
            S_df = pd.DataFrame(S.astype(float))
            S_df.to_csv(path+i+'_S'+'.csv', sep=',', header=False, index=False)
            VT_df = pd.DataFrame(VT.astype(float))
            VT_df.to_csv(path+i+'_VT'+'.csv', sep=',', header=False, index=False)
            ML_df = pd.DataFrame(mean_loadings.astype(float))
            ML_df.to_csv(path+i+'_mean_loadings'+'.csv', sep=',', header=False,index=False)
            stdm_df = pd.DataFrame(stdm.astype(float))
            stdm_df.to_csv(path+i+'_stdm'+'.csv', sep=',', header=False, index=False)
            var_expl_df = pd.DataFrame(var_explained.astype(float))
            var_expl_df.to_csv(path+i+'_stdm'+'.csv', sep=',', header=False, index=False)
            reshaped_df = pd.DataFrame(reshaped_df.astype(float))
            reshaped_df.to_csv(path+i+'_corrected_trials-xyz'+'.csv', sep=',', index=False)
        except KeyError:
            print("""passed: {pos_of_interest} -> check dataframe!""".format(pos_of_interest=pos_of_interest[idx]))


def analysis_example_pos_and_size(event_file,data_file,marker, combination_of_interest, hand, path):
    '''
    Function to analyse movement data based on position and size of target.
    Links functions
        - group_by_position_2H - group data based on position
        - get corrected - zero-center positional data
        - reshape - transform matrix to necessary format for svd
        - extract_mean_factor_loading - perform svd and extract mean factor loading of u1
        - plot_mean_loadings - plot mean factor loading

    input:
    event_file: file containing specified events and their time stamps
    datafile: containing positional data of markers in x,y,z dimensions
    marker = name of marker of interest as list of strings (marker-names in data_file.columns)
    combination_of_interest = abbreavtaion for name of target position and size
                            of interest as list of strings (e.g s1_r1, s2_m1 etc.)
    hand = hand of interest either "r" or "l" as string
    path = path to save data to

    output:
    mean_trajectories = csv file containing mean trajectory of marker for target position
    U_df = csv file (n*n matrix) containing left singular vectors
    S_df = csv file (m*m diagonal matrix) containing singular values
    VT_df = csv file (m*m matrix) containing right singular vectors
    ML_df = csv file of mean loadings on u1 over all trials
    stdm_df = csv file of standard deviation of mean loadings
    reshaped_df = Dataframe with x,y, z dimensions as columns and
    sample*trials as rows

    '''
    # create subj directory
    path = path + '2H/'+hand +'/pos_and_size/' + (marker[0][0:9])+'/'
    try:
        os.makedirs(path)
    except OSError as e:
        if e.errno != errno.EEXIST:
            raise

    s1_r1,s1_r2,s1_r3,s1_m1, s1_m2,s1_m3, s1_l1,s1_l2,s1_l3, s2_r1,s2_r2,s2_r3,s2_m1,s2_m2,s2_m3, s2_l1,s2_l2,s2_l3,s3_r1,s3_r2,s3_r3,s3_m1, s3_m2,s3_m3, s3_l1,s3_l2,s3_l3 = group_by_size_and_pos_2H(event_file, hand)
    for i in combination_of_interest:
        print(i)
        try:
            if i == 's1_r1':
                df_x, df_y, df_z = extract_and_resample_2H(s1_r1, data_file, marker,hand)
                x_mean_dimension, x_variance_dimension, x_df_corrected = get_corrected('x', df_x)
                y_mean_dimension, y_variance_dimension, y_df_corrected = get_corrected('y', df_y)
                z_mean_dimension, z_variance_dimension, z_df_corrected = get_corrected('z', df_z)
                # to csv
                reshaped_df = reshape(x_df_corrected, y_df_corrected, z_df_corrected)
                U, s, S, VT, mean_loadings, stdm, concatenated = extract_mean_factor_loading(reshaped_df)
                # to csv
                fig = plot_mean_loadings(mean_loadings)
            elif i == 's1_r2':
                df_x, df_y, df_z = extract_and_resample_2H(s1_r2, data_file, marker,hand)
                x_mean_dimension, x_variance_dimension, x_df_corrected = get_corrected('x', df_x)
                y_mean_dimension, y_variance_dimension, y_df_corrected = get_corrected('y', df_y)
                z_mean_dimension, z_variance_dimension, z_df_corrected = get_corrected('z', df_z)
                # to csv
                reshaped_df = reshape(x_df_corrected, y_df_corrected, z_df_corrected)
                U, s, S, VT, mean_loadings, stdm, concatenated = extract_mean_factor_loading(reshaped_df)
                # to csv
                fig = plot_mean_loadings(mean_loadings)
            elif i == 's1_r3':
                df_x, df_y, df_z = extract_and_resample_2H(s1_r3, data_file, marker, hand)
                x_mean_dimension, x_variance_dimension, x_df_corrected = get_corrected('x', df_x)
                y_mean_dimension, y_variance_dimension, y_df_corrected = get_corrected('y', df_y)
                z_mean_dimension, z_variance_dimension, z_df_corrected = get_corrected('z', df_z)
                # to csv
                reshaped_df = reshape(x_df_corrected, y_df_corrected, z_df_corrected)
                U, s, S, VT, mean_loadings, stdm, concatenated = extract_mean_factor_loading(reshaped_df)
                # to csv
                fig = plot_mean_loadings(mean_loadings)
            elif i == 's1_m1':
                df_x, df_y, df_z = extract_and_resample_2H(s1_m1, data_file, marker,hand)
                x_mean_dimension, x_variance_dimension, x_df_corrected = get_corrected('x', df_x)
                y_mean_dimension, y_variance_dimension, y_df_corrected = get_corrected('y', df_y)
                z_mean_dimension, z_variance_dimension, z_df_corrected = get_corrected('z', df_z)
                # to csv
                reshaped_df = reshape(x_df_corrected, y_df_corrected, z_df_corrected)
                U, s, S, VT, mean_loadings, stdm, concatenated = extract_mean_factor_loading(reshaped_df)
                # to csv
                fig = plot_mean_loadings(mean_loadings)
            elif i == 's1_m2':
                df_x, df_y, df_z = extract_and_resample_2H(s1_m2, data_file, marker,hand)
                x_mean_dimension, x_variance_dimension, x_df_corrected = get_corrected('x', df_x)
                y_mean_dimension, y_variance_dimension, y_df_corrected = get_corrected('y', df_y)
                z_mean_dimension, z_variance_dimension, z_df_corrected = get_corrected('z', df_z)
                # to csv
                reshaped_df = reshape(x_df_corrected, y_df_corrected, z_df_corrected)
                U, s, S, VT, mean_loadings, stdm, concatenated = extract_mean_factor_loading(reshaped_df)
                # to csv
                fig = plot_mean_loadings(mean_loadings)
            elif i == 's1_m3':
                df_x, df_y, df_z = extract_and_resample_2H(s1_m3, data_file, marker, hand)
                x_mean_dimension, x_variance_dimension, x_df_corrected = get_corrected('x', df_x)
                y_mean_dimension, y_variance_dimension, y_df_corrected = get_corrected('y', df_y)
                z_mean_dimension, z_variance_dimension, z_df_corrected = get_corrected('z', df_z)
                # to csv
                reshaped_df = reshape(x_df_corrected, y_df_corrected, z_df_corrected)
                U, s, S, VT, mean_loadings, stdm, concatenated = extract_mean_factor_loading(reshaped_df)
                # to csv
                fig = plot_mean_loadings(mean_loadings)
            elif i == 's1_l1':
                df_x, df_y, df_z = extract_and_resample_2H(s1_l1, data_file, marker, hand)
                x_mean_dimension, x_variance_dimension, x_df_corrected = get_corrected('x', df_x)
                y_mean_dimension, y_variance_dimension, y_df_corrected = get_corrected('y', df_y)
                z_mean_dimension, z_variance_dimension, z_df_corrected = get_corrected('z', df_z)
                # to csv
                reshaped_df = reshape(x_df_corrected, y_df_corrected, z_df_corrected)
                U, s, S, VT, mean_loadings, stdm, concatenated = extract_mean_factor_loading(reshaped_df)
                # to csv
                fig = plot_mean_loadings(mean_loadings)
            elif i == 's1_l2':
                df_x, df_y, df_z = extract_and_resample_2H(s1_l2, data_file, marker, hand)
                x_mean_dimension, x_variance_dimension, x_df_corrected = get_corrected('x', df_x)
                y_mean_dimension, y_variance_dimension, y_df_corrected = get_corrected('y', df_y)
                z_mean_dimension, z_variance_dimension, z_df_corrected = get_corrected('z', df_z)
                # to csv
                reshaped_df = reshape(x_df_corrected, y_df_corrected, z_df_corrected)
                U, s, S, VT, mean_loadings, stdm, concatenated = extract_mean_factor_loading(reshaped_df)
                # to csv
                fig = plot_mean_loadings(mean_loadings)
            elif i == 's1_l3':
                df_x, df_y, df_z = extract_and_resample_2H(s1_l3, data_file, marker, hand)
                x_mean_dimension, x_variance_dimension, x_df_corrected = get_corrected('x', df_x)
                y_mean_dimension, y_variance_dimension, y_df_corrected = get_corrected('y', df_y)
                z_mean_dimension, z_variance_dimension, z_df_corrected = get_corrected('z', df_z)
                # to csv
                reshaped_df = reshape(x_df_corrected, y_df_corrected, z_df_corrected)
                U, s, S, VT, mean_loadings, stdm, concatenated = extract_mean_factor_loading(reshaped_df)
                # to csv
                fig = plot_mean_loadings(mean_loadings)

            elif i == 's2_r1':
                df_x, df_y, df_z = extract_and_resample_2H(s2_r1, data_file, marker, hand)
                x_mean_dimension, x_variance_dimension, x_df_corrected = get_corrected('x', df_x)
                y_mean_dimension, y_variance_dimension, y_df_corrected = get_corrected('y', df_y)
                z_mean_dimension, z_variance_dimension, z_df_corrected = get_corrected('z', df_z)
                # to csv
                reshaped_df = reshape(x_df_corrected, y_df_corrected, z_df_corrected)
                U, s, S, VT, mean_loadings, stdm, concatenated = extract_mean_factor_loading(reshaped_df)
                # to csv
                fig = plot_mean_loadings(mean_loadings)
            elif i == 's2_r2':
                df_x, df_y, df_z = extract_and_resample_2H(s2_r2, data_file, marker, hand)
                x_mean_dimension, x_variance_dimension, x_df_corrected = get_corrected('x', df_x)
                y_mean_dimension, y_variance_dimension, y_df_corrected = get_corrected('y', df_y)
                z_mean_dimension, z_variance_dimension, z_df_corrected = get_corrected('z', df_z)
                # to csv
                reshaped_df = reshape(x_df_corrected, y_df_corrected, z_df_corrected)
                U, s, S, VT, mean_loadings, stdm, concatenated = extract_mean_factor_loading(reshaped_df)
                # to csv
                fig = plot_mean_loadings(mean_loadings)
            elif i == 's2_r3':
                df_x, df_y, df_z = extract_and_resample_2H(s2_r3, data_file, marker, hand)
                x_mean_dimension, x_variance_dimension, x_df_corrected = get_corrected('x', df_x)
                y_mean_dimension, y_variance_dimension, y_df_corrected = get_corrected('y', df_y)
                z_mean_dimension, z_variance_dimension, z_df_corrected = get_corrected('z', df_z)
                # to csv
                reshaped_df = reshape(x_df_corrected, y_df_corrected, z_df_corrected)
                U, s, S, VT, mean_loadings, stdm, concatenated = extract_mean_factor_loading(reshaped_df)
                # to csv
                fig = plot_mean_loadings(mean_loadings)

            elif i == 's2_m1':
                df_x, df_y, df_z = extract_and_resample_2H(s2_m1, data_file, marker, hand)
                x_mean_dimension, x_variance_dimension, x_df_corrected = get_corrected('x', df_x)
                y_mean_dimension, y_variance_dimension, y_df_corrected = get_corrected('y', df_y)
                z_mean_dimension, z_variance_dimension, z_df_corrected = get_corrected('z', df_z)
                # to csv
                reshaped_df = reshape(x_df_corrected, y_df_corrected, z_df_corrected)
                U, s, S, VT, mean_loadings, stdm, concatenated = extract_mean_factor_loading(reshaped_df)
                # to csv
                fig = plot_mean_loadings(mean_loadings)
            elif i == 's2_m2':
                df_x, df_y, df_z = extract_and_resample_2H(s2_m2, data_file, marker, hand)
                x_mean_dimension, x_variance_dimension, x_df_corrected = get_corrected('x', df_x)
                y_mean_dimension, y_variance_dimension, y_df_corrected = get_corrected('y', df_y)
                z_mean_dimension, z_variance_dimension, z_df_corrected = get_corrected('z', df_z)
                # to csv
                reshaped_df = reshape(x_df_corrected, y_df_corrected, z_df_corrected)
                U, s, S, VT, mean_loadings, stdm, concatenated = extract_mean_factor_loading(reshaped_df)
                # to csv
                fig = plot_mean_loadings(mean_loadings)
            elif i == 's2_m3':
                df_x, df_y, df_z = extract_and_resample_2H(s2_m3, data_file, marker, hand)
                x_mean_dimension, x_variance_dimension, x_df_corrected = get_corrected('x', df_x)
                y_mean_dimension, y_variance_dimension, y_df_corrected = get_corrected('y', df_y)
                z_mean_dimension, z_variance_dimension, z_df_corrected = get_corrected('z', df_z)
                # to csv
                reshaped_df = reshape(x_df_corrected, y_df_corrected, z_df_corrected)
                U, s, S, VT, mean_loadings, stdm, concatenated = extract_mean_factor_loading(reshaped_df)
                # to csv
                fig = plot_mean_loadings(mean_loadings)


            elif i == 's2_l1':
                df_x, df_y, df_z = extract_and_resample_2H(s2_l1, data_file, marker, hand)
                x_mean_dimension, x_variance_dimension, x_df_corrected = get_corrected('x', df_x)
                y_mean_dimension, y_variance_dimension, y_df_corrected = get_corrected('y', df_y)
                z_mean_dimension, z_variance_dimension, z_df_corrected = get_corrected('z', df_z)
                # to csv
                reshaped_df = reshape(x_df_corrected, y_df_corrected, z_df_corrected)
                U, s, S, VT, mean_loadings, stdm, concatenated = extract_mean_factor_loading(reshaped_df)
                # to csv
                fig = plot_mean_loadings(mean_loadings)
            elif i == 's2_l2':
                df_x, df_y, df_z = extract_and_resample_2H(s2_l2, data_file, marker, hand)
                x_mean_dimension, x_variance_dimension, x_df_corrected = get_corrected('x', df_x)
                y_mean_dimension, y_variance_dimension, y_df_corrected = get_corrected('y', df_y)
                z_mean_dimension, z_variance_dimension, z_df_corrected = get_corrected('z', df_z)
                # to csv
                reshaped_df = reshape(x_df_corrected, y_df_corrected, z_df_corrected)
                U, s, S, VT, mean_loadings, stdm, concatenated = extract_mean_factor_loading(reshaped_df)
                # to csv
                fig = plot_mean_loadings(mean_loadings)
            elif i == 's2_l3':
                df_x, df_y, df_z = extract_and_resample_2H(s2_l3, data_file, marker, hand)
                x_mean_dimension, x_variance_dimension, x_df_corrected = get_corrected('x', df_x)
                y_mean_dimension, y_variance_dimension, y_df_corrected = get_corrected('y', df_y)
                z_mean_dimension, z_variance_dimension, z_df_corrected = get_corrected('z', df_z)
                # to csv
                reshaped_df = reshape(x_df_corrected, y_df_corrected, z_df_corrected)
                U, s, S, VT, mean_loadings, stdm, concatenated = extract_mean_factor_loading(reshaped_df)
                # to csv
                fig = plot_mean_loadings(mean_loadings)

            elif i == 's3_r1':
                df_x, df_y, df_z = extract_and_resample_2H(s3_r1, data_file, marker, hand)
                x_mean_dimension, x_variance_dimension, x_df_corrected = get_corrected('x', df_x)
                y_mean_dimension, y_variance_dimension, y_df_corrected = get_corrected('y', df_y)
                z_mean_dimension, z_variance_dimension, z_df_corrected = get_corrected('z', df_z)
                # to csv
                reshaped_df = reshape(x_df_corrected, y_df_corrected, z_df_corrected)
                U, s, S, VT, mean_loadings, stdm, concatenated = extract_mean_factor_loading(reshaped_df)
                # to csv
                fig = plot_mean_loadings(mean_loadings)
            elif i == 's3_r2':
                df_x, df_y, df_z = extract_and_resample_2H(s3_r2, data_file, marker, hand)
                x_mean_dimension, x_variance_dimension, x_df_corrected = get_corrected('x', df_x)
                y_mean_dimension, y_variance_dimension, y_df_corrected = get_corrected('y', df_y)
                z_mean_dimension, z_variance_dimension, z_df_corrected = get_corrected('z', df_z)
                # to csv
                reshaped_df = reshape(x_df_corrected, y_df_corrected, z_df_corrected)
                U, s, S, VT, mean_loadings, stdm, concatenated = extract_mean_factor_loading(reshaped_df)
                # to csv
                fig = plot_mean_loadings(mean_loadings)
            elif i == 's3_r3':
                df_x, df_y, df_z = extract_and_resample_2H(s3_r3, data_file, marker, hand)
                x_mean_dimension, x_variance_dimension, x_df_corrected = get_corrected('x', df_x)
                y_mean_dimension, y_variance_dimension, y_df_corrected = get_corrected('y', df_y)
                z_mean_dimension, z_variance_dimension, z_df_corrected = get_corrected('z', df_z)
                # to csv
                reshaped_df = reshape(x_df_corrected, y_df_corrected, z_df_corrected)
                U, s, S, VT, mean_loadings, stdm, concatenated = extract_mean_factor_loading(reshaped_df)
                # to csv
                fig = plot_mean_loadings(mean_loadings)


            elif i == 's3_m1':
                df_x, df_y, df_z = extract_and_resample_2H(s3_m1, data_file, marker, hand)
                x_mean_dimension, x_variance_dimension, x_df_corrected = get_corrected('x', df_x)
                y_mean_dimension, y_variance_dimension, y_df_corrected = get_corrected('y', df_y)
                z_mean_dimension, z_variance_dimension, z_df_corrected = get_corrected('z', df_z)
                # to csv
                reshaped_df = reshape(x_df_corrected, y_df_corrected, z_df_corrected)
                U, s, S, VT, mean_loadings, stdm, concatenated = extract_mean_factor_loading(reshaped_df)
                # to csv
                fig = plot_mean_loadings(mean_loadings)
            elif i == 's3_m2':
                df_x, df_y, df_z = extract_and_resample_2H(s3_m2, data_file, marker, hand)
                x_mean_dimension, x_variance_dimension, x_df_corrected = get_corrected('x', df_x)
                y_mean_dimension, y_variance_dimension, y_df_corrected = get_corrected('y', df_y)
                z_mean_dimension, z_variance_dimension, z_df_corrected = get_corrected('z', df_z)
                # to csv
                reshaped_df = reshape(x_df_corrected, y_df_corrected, z_df_corrected)
                U, s, S, VT, mean_loadings, stdm, concatenated = extract_mean_factor_loading(reshaped_df)
                # to csv
                fig = plot_mean_loadings(mean_loadings)
            elif i == 's3_m3':
                df_x, df_y, df_z = extract_and_resample_2H(s3_m3, data_file, marker, hand)
                x_mean_dimension, x_variance_dimension, x_df_corrected = get_corrected('x', df_x)
                y_mean_dimension, y_variance_dimension, y_df_corrected = get_corrected('y', df_y)
                z_mean_dimension, z_variance_dimension, z_df_corrected = get_corrected('z', df_z)
                # to csv
                reshaped_df = reshape(x_df_corrected, y_df_corrected, z_df_corrected)
                U, s, S, VT, mean_loadings, stdm, concatenated = extract_mean_factor_loading(reshaped_df)
                # to csv
                fig = plot_mean_loadings(mean_loadings)

            elif i == 's3_l1':
                df_x, df_y, df_z = extract_and_resample_2H(s3_l1, data_file, marker, hand)
                x_mean_dimension, x_variance_dimension, x_df_corrected = get_corrected('x', df_x)
                y_mean_dimension, y_variance_dimension, y_df_corrected = get_corrected('y', df_y)
                z_mean_dimension, z_variance_dimension, z_df_corrected = get_corrected('z', df_z)
                # to csv
                reshaped_df = reshape(x_df_corrected, y_df_corrected, z_df_corrected)
                U, s, S, VT, mean_loadings, stdm, concatenated = extract_mean_factor_loading(reshaped_df)
                # to csv
                fig = plot_mean_loadings(mean_loadings)
            elif i == 's3_l2':
                df_x, df_y, df_z = extract_and_resample_2H(s3_l2, data_file, marker, hand)
                x_mean_dimension, x_variance_dimension, x_df_corrected = get_corrected('x', df_x)
                y_mean_dimension, y_variance_dimension, y_df_corrected = get_corrected('y', df_y)
                z_mean_dimension, z_variance_dimension, z_df_corrected = get_corrected('z', df_z)
                # to csv
                reshaped_df = reshape(x_df_corrected, y_df_corrected, z_df_corrected)
                U, s, S, VT, mean_loadings, stdm, concatenated = extract_mean_factor_loading(reshaped_df)
                # to csv
                fig = plot_mean_loadings(mean_loadings)
            elif i == 's3_l3':
                df_x, df_y, df_z = extract_and_resample_2H(s3_l3, data_file, marker, hand)
                x_mean_dimension, x_variance_dimension, x_df_corrected = get_corrected('x', df_x)
                y_mean_dimension, y_variance_dimension, y_df_corrected = get_corrected('y', df_y)
                z_mean_dimension, z_variance_dimension, z_df_corrected = get_corrected('z', df_z)
                # to csv
                reshaped_df = reshape(x_df_corrected, y_df_corrected, z_df_corrected)
                U, s, S, VT, mean_loadings, stdm, concatenated = extract_mean_factor_loading(reshaped_df)
                # to csv
                fig = plot_mean_loadings(mean_loadings)
            df_x.to_csv(path+i+'_x_dim'+'.csv', sep=',',  float_format='%.2f', index=False)
            df_y.to_csv(path+i+'_y_dim'+'.csv', sep=',',  float_format='%.2f', index=False)
            df_z.to_csv(path+i+'_trials_z_dim'+'.csv', sep=',',  float_format='%.2f', index=False)
            mean_trajectories = pd.DataFrame({'x_mean': x_mean_dimension,'y_mean':y_mean_dimension,
                                             'z_mean': z_mean_dimension, 'x_var':x_variance_dimension,
                                             'y_var':y_variance_dimension,'z_var':z_variance_dimension})
            mean_trajectories.to_csv(path+i+'_mean_trajectories'+'.csv', sep=',',  float_format='%.2f', index=False)
            U_df = pd.DataFrame(data=U.astype(float))
            U_df.to_csv(path+i+'_U'+'.csv', sep=',', header=False, float_format='%.2f', index=False)
            S_df = pd.DataFrame(S.astype(float))
            S_df.to_csv(path+i+'_S'+'.csv', sep=',', header=False, float_format='%.2f', index=False)
            VT_df = pd.DataFrame(VT.astype(float))
            VT_df.to_csv(path+i+'_VT'+'.csv', sep=',', header=False, float_format='%.2f', index=False)
            ML_df = pd.DataFrame(mean_loadings.astype(float))
            ML_df.to_csv(path+i+'_mean_loadings'+'.csv', sep=',', header=False, float_format='%.2f', index=False)
            stdm_df = pd.DataFrame(stdm.astype(float))
            stdm_df.to_csv(path+i+'_stdm'+'.csv', sep=',', header=False, float_format='%.2f', index=False)
            reshaped_df = pd.DataFrame(reshaped_df.astype(float))
            reshaped_df.to_csv(path+i+'_corrected_trials-xyz'+'.csv', sep=',', float_format='%.2f', index=False)
            return ML_df
        except KeyError:
            print('passed')


def test_beg_vs_end(mean_loadings, size, position, marker):
    '''
    Function to compare mean factor loading of u1 (generated by extract_and_resample_2H function)
    at beginning to mean_factor loading at end of trial.
    First tests data for normality (shapiro-wilk-test),
    then performs either a t-test or a wilcoxon signed rank test.

    input:
    mean_loadings = mean loadings on u1 over all trials
    size = size of interest as string ("s1", "s2","s3")
    position = name of target position of interest as string(e.g. r1, m2 etc.)
    marker = name of marker of interest as list of strings (marker-names in data_file.columns)

    output:
    df = Dataframe containing data on size, position, normality, used statistical tests,
         test-statistics and p-values

    '''
    list_normal = []
    list_cat = []
    list_test = []
    list_statistic = []
    list_p_val =  []
    list_interpretation = []

    alpha = 0.05
    print('Shapiro-Wilk-test:')
    s, p = stats.shapiro(mean_loadings)
    stats.probplot(mean_loadings, plot= plt)
    plt.show()

    if p > alpha:
        print('W: '+str(s))
        print('p-value:' +str(p))
        print("""failure to reject H0: data possibly drawn from normal distibution:
        using t-test for paired samples""")
        list_normal.append('failure to reject H0: possibly drawn from normal distibution. p= '+str(p))

        t, p = stats.ttest_rel(mean_loadings[:100], mean_loadings[200:300])

        list_cat.append('beg-vs-end')
        list_test.append('t-test')
        list_statistic.append(s)
        list_p_val.append(p)
        print(p)
        if p < alpha:
            print('W: '+str(t))
            print('p-value:' +str(p))
            print('statistically signifciant difference')
            list_interpretation.append('statistically signifciant difference at (p:{a};t:{b})'.format(a=p,b=t))
        elif p >= alpha:
            print('W: '+str(t))
            print('p-value:' +str(p))
            print('statistically non-signifciant difference')
            list_interpretation.append('statistically non-signifciant difference at (p:{a};t:{b})'.format(a=p,b=t))
        elif np.isnan(p) == True :
            print('W: '+str(t))
            print('p-value:' +str(p))
            print('nan: not applicable')
            list_interpretation.append('nan: not applicable')

    elif p <= alpha:
        print('W :'+str(s))
        print('p-value:' +str(p))
        print("""reject H0: data possibly drawn from non-normal distibution:
        using wilcoxon""")
        list_normal.append('reject H0: possibly drawn from non-normal distibution. p= '+str(p))

        # The Wilcoxon signed-rank test tests the null hypothesis that two related paired samples
        # come from the same distribution.
        #In particular, it tests whether the distribution of the differences x - y is symmetric about zero
#         display(stats.wilcoxon(mean_loadings[40:60], mean_loadings[80:100]))
        print('++++++++++++++++++++++++++++++ beginning vs end +++++++++++++++++++++++++++++++++++++')
        w, p = stats.wilcoxon(mean_loadings[:100], mean_loadings[200:300])
        list_cat.append('beg-vs-end')
        list_test.append('wilcoxon-signed-rank')
        list_statistic.append(w)
        list_p_val.append(p)
        print(p)
        if p < alpha:
            print('W: '+str(w))
            print('p-value:' +str(p))
            print('statistically signifciant difference')
            #If the P value is small, you can reject the idea that the difference is
            #due to chance and conclude instead that the population has a median distinct
            # from the hypothetical value you entered.
            list_interpretation.append('statistically signifciant difference at (p:{a};t:{b})'.format(a=p,b=w))

        elif p >= alpha:
            print('W: '+str(w))
            print('p-value:' +str(p))
            print('statistically non-signifciant difference')
            list_interpretation.append('statistically non-signifciant difference at (p:{a};t:{b})'.format(a=p,b=w))
        elif np.isnan(p) == True :
            print('W: '+str(w))
            print('p-value:' +str(p))
            print('nan: not applicable')
            list_interpretation.append('nan: not applicable')


    list_marker = []
    list_size = []
    list_pos = []
    for i,_ in enumerate(list_p_val):
        list_marker.append(marker)
        list_size.append(size)
        list_pos.append(position)

    df = pd.DataFrame({'size':list_size, 'position':list_pos,'marker':list_marker,'tested':list_cat, 'normally_distributed':list_normal,
                       'p_val':list_p_val,
                       'test-statistic':list_statistic, 'interpretation':list_interpretation})
    return df
# df_output = pd.DataFrame({'size':size, 'position':pos,'category':cat,'p_val':p, 'test-statistic':w})

def test_middle_vs_end(mean_loadings, size, position, marker):
    '''
    Function to compare mean factor loading of u1 (generated by extract_and_resample_2H function)
    at middle of trial to mean_factor loading at end of trial.
    First tests data for normality (shapiro-wilk-test),
    then performs either a t-test or a wilcoxon signed rank test.

    input:
    mean_loadings = mean loadings on u1 over all trials
    size = size of interest as string ("s1", "s2","s3")
    position = name of target position of interest as string(e.g. r1, m2 etc.)
    marker = name of marker of interest as list of strings (marker-names in data_file.columns)

    output:
    df = Dataframe containing data on size, position, normality, used statistical tests,
         test-statistics and p-values

    '''
    list_normal = []
    list_cat = []
    list_test = []
    list_statistic = []
    list_p_val =  []
    list_interpretation = []
    alpha = 0.05
    print('Shapiro-Wilk-test:')
    s, p = stats.shapiro(mean_loadings)
    stats.probplot(mean_loadings, plot= plt)
#     plt.boxplot(mean_loadings)
    plt.show()
    print(p)
    if p > alpha:
        print('W: '+str(s))
        print('p-value:' +str(p))
        print("""failure to reject H0: data possibly drawn from normal distibution:
        using t-test for paired samples""")
        list_normal.append('failure to reject H0: possibly drawn from normal distibution. p= '+str(p))

        t, p = stats.ttest_rel(mean_loadings[100:200], mean_loadings[200:300])
        list_cat.append('beg-vs-end')
        list_test.append('t-test')
        list_statistic.append(s)
        list_p_val.append(p)
        print(p)
        if p < alpha:
            print('W: '+str(t))
            print('p-value:' +str(p))
            print('statistically signifciant difference')
            list_interpretation.append('statistically signifciant difference at (p:{a};t:{b})'.format(a=p,b=t))
        elif p >= alpha:
            print('W: '+str(t))
            print('p-value:' +str(p))
            print('statistically non-signifciant difference')
            list_interpretation.append('statistically non-signifciant difference at (p:{a};t:{b})'.format(a=p,b=t))
        elif np.isnan(p) == True :
            print('W: '+str(t))
            print('p-value:' +str(p))
            print('nan: not applicable')
            list_interpretation.append('nan: not applicable')

    elif p <= alpha:
        print('W :'+str(s))
        print('p-value:' +str(p))
        print("""reject H0: data possibly drawn from non-normal distibution:
        using wilcoxon""")
        list_normal.append('reject H0: possibly drawn from non-normal distibution. p= '+str(p))

        # The Wilcoxon signed-rank test tests the null hypothesis that two related paired samples
        # come from the same distribution.
        #In particular, it tests whether the distribution of the differences x - y is symmetric about zero
#         display(stats.wilcoxon(mean_loadings[40:60], mean_loadings[80:100]))
        print('++++++++++++++++++++++++++++++ middle vs end +++++++++++++++++++++++++++++++++++++')
        w, p = stats.wilcoxon(mean_loadings[100:200], mean_loadings[200:300])
        list_cat.append('middle-vs-end')
        list_test.append('wilcoxon-signed-rank')
        list_statistic.append(w)
        list_p_val.append(p)
        print(p)
        if p < alpha:
            print('W: '+str(w))
            print('p-value:' +str(p))
            print('statistically signifciant difference')
            #If the P value is small, you can reject the idea that the difference is
            #due to chance and conclude instead that the population has a median distinct
            # from the hypothetical value you entered.
            list_interpretation.append('statistically signifciant difference at (p:{a};t:{b})'.format(a=p,b=w))

        elif p >= alpha:
            print('W: '+str(w))
            print('p-value:' +str(p))
            print('statistically non-signifciant difference')
            list_interpretation.append('statistically signifciant difference at (p:{a};t:{b})'.format(a=p,b=t))

        elif np.isnan(p) == True :
            print('W: '+str(w))
            print('p-value:' +str(p))
            print('nan: not applicable')
            list_interpretation.append('nan: not applicable')

    list_marker = []
    list_size = []
    list_pos = []
    for i,_ in enumerate(list_p_val):
        list_marker.append(marker)
        list_size.append(size)
        list_pos.append(position)

    df = pd.DataFrame({'size':list_size, 'position':list_pos,'marker':list_marker,'tested':list_cat, 'normally_distributed':list_normal,
                       'p_val':list_p_val,
                       'test-statistic':list_statistic, 'interpretation':list_interpretation})
    return df
# df_output = pd.DataFrame({'size':size, 'position':pos,'category':cat,'p_val':p, 'test-statistic':w})



def test_a_vs_b(mean_loadings_a,mean_loadings_b, size_a,size_b, position_a,position_b, marker_a,marker_b):
    '''
    Function to compare mean factor loading of u1 (generated by extract_and_resample_2H function)
    of 2 different markers.
    First tests data for normality (shapiro-wilk-test),
    then performs either a t-test or a wilcoxon signed rank test.

    input:
    mean_loadings = mean loadings on u1 over all trials
    size = size of interest as string ("s1", "s2","s3")
    position = name of target position of interest as string(e.g. r1, m2 etc.)
    marker = name of marker of interest as list of strings (marker-names in data_file.columns)

    output:
    df = Dataframe containing data on size, position, normality, used statistical tests,
         test-statistics and p-values

    '''
    list_normal_a = []
    list_normal_b = []
    list_cat = []
    list_test = []
    list_statistic = []
    list_p_val =  []
    list_interpretation = []



    print('Shapiro-Wilk-test:')
    alpha = 0.05
    s, p = stats.shapiro(mean_loadings_a)
    p = '{:f}'.format(p)
    p = float(p)

    s_, p_ = stats.shapiro(mean_loadings_b)
    p_ = '{:f}'.format(p_)

    p_ = float(p_)
    stats.probplot(mean_loadings_a, plot= plt)

    plt.title('A')
    plt.show()

    stats.probplot(mean_loadings_b, plot= plt)
#     plt.boxplot(mean_loadings)
    plt.title('B')
    plt.show()
    print(p)

    if p > alpha and p_ > alpha:
        print('W: '+str(s))
        print('p-value:' +str(p))
        print("""failure to reject H0: data possibly drawn from normal distibution:
        using t-test for paired samples""")
        list_normal_a.append('failure to reject H0: possibly drawn from normal distibution. p= '+str(p))
        list_normal_b.append('failure to reject H0: possibly drawn from normal distibution. p= '+str(p_))

        # Calculates the T-test on TWO RELATED samples of scores, a and b.
        # This is a two-sided test for the null hypothesis that
        # 2 related or repeated samples have identical average (expected) values
        t, p = stats.ttest_rel(mean_loadings_a, mean_loadings_b)

        list_cat.append('beg-vs-end')
        list_test.append('t-test')
        list_statistic.append(s)
        list_p_val.append(p)
        print(p)

        if p < alpha:
            print('W: '+str(t))
            print('p-value:' +str(p))
            print('statistically signifciant difference')
            list_interpretation.append('statistically signifciant difference at (p:{a};t:{b})'.format(a=p,b=t))

        elif p >= alpha:
            print('W: '+str(t))
            print('p-value:' +str(p))
            print('statistically non-signifciant difference')
            list_interpretation.append('statistically non-signifciant difference at (p:{a};t:{b})'.format(a=p,b=t))

        elif np.isnan(p) == True :
            print('W: '+str(t))
            print('p-value:' +str(p))
            print('nan: not applicable')
            list_interpretation.append('nan: not applicable')

    elif p <= alpha and p_ <= alpha:
        print('W :'+str(s))
        print('p-value:' +str(p))
        print("""reject H0: data possibly drawn from non-normal distibution:
        using wilcoxon""")

        list_normal_a.append('reject H0: possibly drawn from non-normal distibution. p= '+str(p))
        list_normal_b.append('reject H0: possibly drawn from non-normal distibution. p= '+str(p_))

        # The Wilcoxon signed-rank test tests the null hypothesis that two related paired samples
        # come from the same distribution.
        #In particular, it tests whether the distribution of the differences x - y is symmetric about zero
#         display(stats.wilcoxon(mean_loadings[40:60], mean_loadings[80:100]))
        print('++++++++++++++++++++++++++++++ a vs b +++++++++++++++++++++++++++++++++++++')
        w, p = stats.wilcoxon(mean_loadings_a,mean_loadings_b)

        list_cat.append(size_a+' '+position_a+' vs '+size_b+' '+position_b)
        list_test.append('wilcoxon-signed-rank')
        list_statistic.append(w)
        list_p_val.append(p)
        print(p)

        if p < alpha:
            print('W: '+str(w))
            print('p-value:' +str(p))
            print('statistically signifciant difference')
            #If the P value is small, you can reject the idea that the difference is
            #due to chance and conclude instead that the population has a median distinct
            # from the hypothetical value you entered.
            list_interpretation.append('statistically signifciant difference at (p:{a};t:{b})'.format(a=p,b=w))

        elif p >= alpha:
            print('W: '+str(w))
            print('p-value:' +str(p))
            print('statistically non-signifciant difference')
            list_interpretation.append('statistically signifciant difference at (p:{a};t:{b})'.format(a=p,b=t))

        elif np.isnan(p) == True :
            print('W: '+str(w))
            print('p-value:' +str(p))
            print('nan: not applicable')
            list_interpretation.append('nan: not applicable')
    else:
        print('distributions do not match: abort')

    list_marker_a = []
    list_size_a = []
    list_pos_a = []
    list_marker_b = []
    list_size_b = []
    list_pos_b = []
    for i,_ in enumerate(list_p_val):
        list_marker_a.append(marker_a)
        list_size_a.append(size_a)
        list_pos_a.append(position_a)
        list_marker_b.append(marker_b)
        list_size_b.append(size_b)
        list_pos_b.append(position_b)


    df = pd.DataFrame({'size_a':list_size_a,'size_b':list_size_b,
                       'position_a':list_pos_a,'position_b':list_pos_b,
                       'marker_a':list_marker_a,'marker_b':list_marker_b,
                       'tested':list_cat,
                       'normally_distributed_a':list_normal_a,'normally_distributed_b':list_normal_b,
                       'p_val':list_p_val,
                       'test-statistic':list_statistic, 'interpretation':list_interpretation})
    return df

# specify labels for each marker
Marker_0_id = 'head_1'
Marker_1_id = 'head_2'
Marker_2_id = 'head_3'
Marker_3_id = 'head_4'
Marker_4_id = 'left_hand_1'
Marker_5_id = 'left_hand_2'
Marker_6_id = 'left_hand_3'
Marker_7_id = 'right_hand_1'
Marker_8_id = 'right_hand_2'
Marker_9_id = 'right_hand_3'
Marker_10_id = 'chest'
Marker_11_id = 'stomach'
Marker_12_id = 'upper_back_central'
Marker_13_id = 'lower_back_central'
Marker_14_id = 'right_back_hip'
Marker_15_id = 'left_back_hip'
Marker_16_id = 'left_shoulder'
Marker_17_id = 'left_upper_arm'
Marker_18_id = 'left_lower_arm'
Marker_19_id = 'left_wrist'
Marker_20_id = 'right_shoulder'
Marker_21_id = 'right_upper_arm'
Marker_22_id = 'right_lower_arm'
Marker_23_id = 'right_wrist'
Marker_24_id = 'left_foot_1'
Marker_25_id = 'left_foot_2'
Marker_26_id = 'left_foot_3'
Marker_27_id = 'left_lower_leg'
Marker_28_id = 'left_knee'
Marker_29_id = 'left_upper_leg'
Marker_30_id = 'left_front_hip'
Marker_31_id = 'right_front_hip'
Marker_32_id = 'right_upper_leg'
Marker_33_id = 'right_knee'
Marker_34_id = 'right_lower_leg'
Marker_35_id = 'right_foot_1'
Marker_36_id = 'right_foot_2'
Marker_37_id = 'right_foot_3'

left_arm_marker = [4,5,6,16,17,18,19]
left_hand_marker = [4,5,6]
left_shoulder = [16]
left_upper_arm = [17]
left_lower_arm = [18]
left_wrist = [19]
right_arm_marker = [7,8,9,20,21,22,23]
right_hand_marker = [7,8,9]
right_shoulder = [20]
right_upper_arm = [21]
right_lower_arm = [22]
right_wrist = [23]
head_marker = [0,1,2,3]
torso_marker = [10,11,12,13,14,1530,31]
chest = [10]
stomach = [11]
left_leg = [27,28,29]
left_knee = [28]
left_foot = [24,25,26]
right_leg = [32,33,34]
right_knee = [33]
right_foot = [35,36,37]

keys = list(range(38))
labels = [Marker_0_id, Marker_1_id,Marker_2_id,
            Marker_3_id, Marker_4_id, Marker_5_id,
            Marker_6_id, Marker_7_id, Marker_8_id,
            Marker_9_id, Marker_10_id, Marker_11_id,
            Marker_12_id, Marker_13_id, Marker_14_id,
            Marker_15_id, Marker_16_id, Marker_17_id,
            Marker_18_id, Marker_19_id, Marker_20_id,
            Marker_21_id, Marker_22_id, Marker_23_id,
            Marker_24_id, Marker_25_id, Marker_26_id,
            Marker_27_id, Marker_28_id, Marker_29_id,
            Marker_30_id, Marker_31_id, Marker_32_id,
            Marker_33_id, Marker_34_id, Marker_35_id,
            Marker_36_id, Marker_37_id]

marker_dict = dict(zip(keys, labels))
